[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Publications",
    "section": "",
    "text": "2021\nSchweinsberg, Martin, Feldman, Michael, Staub, Nicola, van den Akker, Olmo R, van Aert, Robbie CM, Van Assen, Marcel ALM, Liu, Yang, Althoff, Tim, Heer, Jeffrey, Kale, Alex, Michael Schulte-Mecklenbeck, and others.. (2021) \"Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis.\" Organizational Behavior and Human Decision Processes.\n        \n        Paper\n    \n2020\nClaudio Georgii, Michael Schulte-Mecklenbeck, Anna Richard, Zo√© Van Dyck, and Jens Blechert. (2020) \"The dynamics of self-control: within-participant modeling of binary food choices and underlying decision processes as a function of restrained eating.\" Psychological Research.\n        \n        Paper\n    \n2019\nTomas Lejarraga, Michael Schulte-Mecklenbeck, Thorsten Pachur, and Ralph Hertwig. (2019) \"The attention‚Äìaversion gap: how allocation of attention relates to loss aversion.\" Evolution and Human Behavior.\n        \n        Paper\n    \nPJ Kieslich, F Henninger, DU Wulff, JMB Haslbeck, and Michael Schulte-Mecklenbeck. (2019) \"Mouse-tracking: A practical guide to implementation and analysis.\" A Handbook of Process Tracing Methods.\n        \n        Paper\n    \nJonathan Tennant, Jennifer Beamer, Jeroen Bosman, Bj√∂rn Brembs, Neo Christopher Chung, Gail Clement, Tom Crick, Jonathan Dugan, Alastair Dunning, David Eccles, Michael Schulte-Mecklenbeck, and others. (2019) \"Foundations for Open Scholarship Strategy Development.\" OSF.\n        \n        Paper\n    \nMartin Schoemann, Michael Schulte-Mecklenbeck, Frank Renkewitz, and Stefan Scherbaum. (2019) \"Forward inference in risky choice: Mapping gaze and decision processes.\" Journal of Behavioral Decision Making.\n        \n        Paper\n     \n        \n        Github\n    \n2018\nAnton Kuehberger and Michael Schulte-Mecklenbeck. (2018) \"Selecting target papers for replication.\" Behavioral and Brain Sciences.\n        \n        Paper\n    \nMichael O‚ÄôDonnell, Leif D Nelson, Evi Ackermann, Balazs Aczel, Athfah Akhtar, Michael Schulte-Mecklenbeck, and others. (2018) \"Registered replication report: Dijksterhuis and van Knippenberg (1998).\" Perspectives on Psychological Science.\n        \n        Paper\n    \nSabrina St√∂ckli, Michael Schulte-Mecklenbeck, Stefan Borer, and Andrea C Samson. (2018) \"Facial expression analysis with AFFDEX and FACET: A validation study.\" Behavior research methods.\n        \n        Paper\n    \nEmanuel de Bellis, Michael Schulte-Mecklenbeck, Wernher Brucks, Andreas Herrmann, and Ralph Hertwig. (2018) \"Blind haste: As light decreases, speeding increases.\" PLoS one.\n        \n        Paper\n    \nBraeden Hall, Jordan Wagge, Christopher R Chartier, Gerit Pfuhl, Stefan Stieger, Evie Vergauwe, Erica D Musser, Leslie Cramblet Alvarez, Michael Schulte-Mecklenbeck, and others. (2018) \"Registered Replication Report: A Large Multilab Cross-Cultural Conceptual Replication of Turri, Buckwalter, & Blouw (2015).\" OSF.\n        \n        Paper\n    \nSophia Cr√ºwell, Johnny van Doorn, Sophia Cr√ºwell, Johnny van Doorn, Alexander Etz, Matthew C Makel, Hannah Moshontz, Jesse Niebaum, Amy Orben, Sam Parsons, and Michael Schulte-Mecklenbeck. (2018) \"8 Easy Steps to Open Science: An Annotated Reading List (To Give to your busy Supervisor/Office Mate).\" Open Science.\n        \n        Paper\n    \n2017\nMichael Schulte-Mecklenbeck, Nanon L Spaanjaars, and Cilia LM Witteman. (2017) \"The (in) visibility of psychodiagnosticians expertise.\" Journal of behavioral decision making.\n        \n        Paper\n    \nThorsten Pachur, Michael Schulte-Mecklenbeck, Ryan O Murphy, and Ralph Hertwig. (2017) \"Prospect Theory Reflects Selective Allocation of Attention.\" Journal of Experimental Psychology: General.\n        \n        Paper\n    \nMichael Schulte-Mecklenbeck, J.G. Johnson, U. B√∂ckenholt, D. Goldstein, J. Russo, N. Sullivan, and M. Willemsen. (2017) \"Process tracing methods in decision making: on growing up in the 70ties.\" Current Directions in Psychological Science.\n        \n        Paper\n    \nMichael Schulte-Mecklenbeck, Anton K√ºhberger, Benjamin Gagl, and Florian Hutzler. (2017) \"Inducing Thought Processes: Bringing Process Measures and Cognitive Processes Closer Together.\" Journal of Behavioral Decision Making.\n        \n        Paper\n    \n2016\nTomas Lejarraga, Michael Schulte-Mecklenbeck, and Daniel Smedema. (2016) \"The pyeTribe: Simultaneous eyetracking for economic games.\" Behavior Research Methods.\n        \n        Paper\n    \nAleksandrina Skvortsova, Michael Schulte-Mecklenbeck, Sophie Jellema, Alan Sanfey, and Cilia Witteman. (2016) \"Deliberative versus intuitive diagnostic decision.\" Psychology.\n        \n        Paper\n    \n2014\nMichael Schulte-Mecklenbeck and Anton K√ºhberger. (2014) \"Out of sight--out of mind? Information acquisition patterns in risky choice framing.\" Polish Psychological Bulletin.\n        \n        Paper\n    \n2013\nMichael Schulte-Mecklenbeck, Matthias Sohn, Emanuel de Bellis, Nathalie Martin, and Ralph Hertwig. (2013) \"A Lack of Appetite for Information and Computation: Simple Heuristics in Food Choice.\" Appetite.\n        \n        Paper\n    \n2012\nMichael Schulte-Mecklenbeck and Ryan O Murphy. (2012) \"Flashlight as a process tracing method.\" Encyclopedia of cyber behavior.\n        \n        Paper\n    \n2011\nMichael Schulte-Mecklenbeck, Anton K√ºhberger, and Rob Ranyard. (2011) \"The role of process data in the development and testing of process models of judgment and decision making.\" Judgment & Decision Making.\n        \n        Paper\n    \nMichael Schulte-Mecklenbeck, Ryan O Murphy, and Florian Hutzler. (2011) \"Flashlight--Recording information acquisition online.\" Computers in human behavior.\n        \n        Paper\n    \nOswald Huber, Odilo W Huber, and Michael Schulte-Mecklenbeck. (2011) \"Determining the information participants need: methods of active information search.\" A handbook of process tracing methods for decision research.\n        \n        Paper\n    \nMichael Schulte-Mecklenbeck, Anton K√ºhberger, and Rob Ranyard. (2011) \"A handbook of process tracing methods for decision research: A critical review and user‚Äôs guide.\"\n2010\nA Kuhberger, Michael Schulte-Mecklenbeck, and Rob Ranyard. (2010) \"Windows for understanding the mind: Introduction to a handbook of process tracing methods for decision research.\" A handbook of process tracing methods for decision research: A critical review and user‚Äôs guide.\nBernd Figner, Ryan O Murphy, Michael Schulte-Mecklenbeck, Anton Kuehberger, and Rob Ranyard. (2010) \"None.\" A handbook of process tracing methods for decision research: A critical review and user‚Äôs guide.\n2009\nElisabeth Norman and Michael Schulte-Mecklenbeck. (2009) \"Take a quick click at that! Mouselab and eye-tracking as tools to measure intuition.\" Foundations for tracing intuition.\n        \n        Paper\n    \n2008\nEric J Johnson, Michael Schulte-Mecklenbeck, and Martijn C Willemsen. (2008) \"Process models deserve process data: Comment on Brandst√§tter, Gigerenzer, and Hertwig (2006).\" Psychological Review.\n        \n        Paper\n    \n2005\nMichael Schulte-Mecklenbeck and Moritz Neun. (2005) \"WebDiP: A tool for information search experiments on the World-Wide Web.\" Behavior Research Methods.\n        \n        Paper\n    \nMichael Schulte-Mecklenbeck. (2005) \"Brave New World... Wide Web: Blending Old Teaching.\" APS Observer.\n        \n        Paper\n    \n2003\nMichael Schulte-Mecklenbeck and Oswald Huber. (2003) \"Information search in the laboratory and on the Web: With or without an experimenter.\" Behavior Research Methods, Instruments, & Computers.\n        \n        Paper\n    \n2002\nAnton K√ºhberger, Michael Schulte-Mecklenbeck, and Josef Perner. (2002) \"Framing decisions: Hypothetical and real.\" Organizational Behavior and Human Decision Processes.\n        \n        Paper\n    \n1999\nAnton K√ºhberger, Michael Schulte-Mecklenbeck, and Josef Perner. (1999) \"The effects of framing, reflection, probability, and payoff on risk preference in choice tasks.\" Organizational behavior and human decision processes.\n        \n        Paper\n    \n1995\nAnton K√ºhberger, Josef Perner, Michael Schulte-Mecklenbeck, and Robert Leingruber. (1995) \"Choice or no choice: is the langer effect evidence against simulation.\" Mind & language.\n        \n        Paper"
  },
  {
    "objectID": "posts/2016-01-28-sublime-autocompletion.html",
    "href": "posts/2016-01-28-sublime-autocompletion.html",
    "title": "Sublime autocompletion",
    "section": "",
    "text": "They say about Sublime: ‚ÄúThe text editor you will fall in love with‚Äù\nhmm ‚Äì kind of did that.\nOne reason being that when coding¬†LaTeX in Sublime there is an awesome autocompletion feature:\nType ‚Äòenum‚Äô and you get:\n\n¬†\n¬†\n¬†\nShift + Enter within an itemize adds a new\n\n¬†\n¬†\n¬†\nAFK to cry ‚Ä¶ tears of joy üôÇ\nEverything stolen from here"
  },
  {
    "objectID": "posts/2020-01-19-open-science-lecture-series.html",
    "href": "posts/2020-01-19-open-science-lecture-series.html",
    "title": "Open Science Lecture Series",
    "section": "",
    "text": "This will be fun - Lecture series on Open Science, Replication, Reproducibility, Open Data and many more.\nTruly interdisciplinary series with speakers from business, psychology, life science, medicine, library sciences and informatics.\nBe there or be ‚Ä¶"
  },
  {
    "objectID": "posts/2022-11-23-kisses/index.html",
    "href": "posts/2022-11-23-kisses/index.html",
    "title": "Replication: Kisses",
    "section": "",
    "text": "Rottenstreich, Y., & Hsee, C. K. (2001). Money, kisses, and electric shocks: On the affective psychology of risk."
  },
  {
    "objectID": "posts/2022-11-23-kisses/index.html#abstract",
    "href": "posts/2022-11-23-kisses/index.html#abstract",
    "title": "Replication: Kisses",
    "section": "Abstract",
    "text": "Abstract\nProspect theory‚Äôs S-shaped weighting function is often said to reflect the psychophysics of chance. We propose an affective rather than psychophysical deconstruction of the weighting function resting on two assumptions. First, preferences depend on the affective reactions associated with potential outcomes of a risky choice. Second, even with monetary values controlled, some outcomes are relatively affect-rich and others relatively affect-poor. Although the psychophysical and affective approaches are complementary, the affective approach has one novel implication: Weighting functions will be more S-shaped for lotteries involving affect-rich than affect-poor outcomes. That is, people will be more sensitive to departures from impossibility and certainty but less sensitive to intermediate probability variations for affect-rich outcomes. We corroborated this prediction by observing probability-outcome interactions: An affect-poor prize was preferred over an affect-rich prize under certainty, but the direction of preference reversed under low probability. We suggest that the assumption of probability-outcome independence, adopted by both expected-utility and prospect theory, may hold across outcomes of different monetary values, but not different affective values."
  },
  {
    "objectID": "posts/2009-06-05-inference-and-r.html",
    "href": "posts/2009-06-05-inference-and-r.html",
    "title": "Inference and R",
    "section": "",
    "text": "Dan Goldstein posted a short overview of Inference which allows working with R code in Microsoft Office and Excel.\nI want to point at Sweave which does an excellent job in connecting R to LaTeX. Here is a short demo of Sweave which also connects the approach of Sweave to the ‚Äòliterate programming‚Äò idea of Donald Knuth (Father of ‚ÄòThe Art of Computer Programming‚Äô and TeX).\nThe basic idea is to combine programming (an analysis in the case of R) with documentation into one process. This seems to be useful when one goes back to an older analysis and tries to find out what was done some months ago.\nAdditionally you find a longer interview with Paul van Eikeren on the same topic here.\nEnjoy!"
  },
  {
    "objectID": "posts/2010-07-13-r-and-the-world-cup.html",
    "href": "posts/2010-07-13-r-and-the-world-cup.html",
    "title": "R and the World Cup",
    "section": "",
    "text": "Across the street at the Revolution blog a nice example of using R with data from the cloud (see another post on this topic here) shows us the distribution of fouls during the just finished World Cup in a nice barchart. Even more interesting than the fact that Holland rules this category is the way the data are collected from a Google spreadsheet page.\nWith the following simple code line:\nteams &lt;- read.csv(\"http://spreadsheets.google.com/pub?key=tOM2qREmPUbv76waumrEEYg&#038;single=true&#038;gid=0&#038;range=A1%3AAG15&#038;output=csv\")\nWe can read a specific part from a spreadsheet hosted on Google into our local R environment. Some deatils: ‚Äú&gid=‚Äù (sheet number) and ‚Äú%range=‚Äù (cell ranges: A1%3A ) and ‚Äú&output=csv‚Äù to download in CSV format.\nWith some more lines, using the awsome ggplot2\n&lt;br /&gt; library(qqplot2)&lt;br /&gt; FOULS=t(DF2)[,c('Fouls')]&lt;br /&gt; qplot(names(FOULS), as.numeric(FOULS), geom=\"bar\", stat='identity', fill=Fouls) + xlab('Country') + ylab('Fouls') + coord_flip() + scale_fill_continuous(low=\"black\", high=\"red\") + labs(fill='Fouls')\nWe can produce the following chart:\n\nTwo things to note:\nc('Fouls') is a handy way to address columns in a R data frame by name\nscale_fill_continuous(low=\"black\", high=\"red\") takes care of the color coding of the bars in reference to the number of fouls\nEasy and straight forward - ah - great job Spain üôÇ !!"
  },
  {
    "objectID": "posts/2016-06-11-the-exams-package.html",
    "href": "posts/2016-06-11-the-exams-package.html",
    "title": "The exams package",
    "section": "",
    "text": "I gave the R package exams¬†a shot for my decision making lecture. Here is what it does:\n‚ÄúAutomatic generation of exams based on exercises in Sweave (R/LaTeX) or R/Markdown format, including multiple-choice questions and arithmetic problems. Exams can be produced in various formats, including PDF, HTML, Moodle XML, QTI 1.2 (for OLAT/OpenOLAT), QTI 2.1, ARSnova, and TCExam. In addition to fully customizable PDF exams, a standardized PDF format is provided that can be printed, scanned, and automatically evaluated.‚Äù\nAfter some fiddling and help from one of the authors (the incredible nice Achim Zeileis,¬†Uni Innsbruck) ¬†I¬†got the following setup going:\n\npool of ~ 100 questions in .Rmd format (all multiple choice, 3-6 answer options) grouped into lectures\nsampling out of the pool (e.g., 5 questions out of each lecture)\nrandom order of questions in each version of the exam (while keeping the lecture order, which I think is useful to give student more structure to work from)\nrandom order of the answers for each question\nexam with the correct answers\n\nThere are three¬†parts:\n\nquestions[] defining the answers to a question\nsolutions[] defining the correct answers\nin LaTeX the actual question\n\nAll of this information goes into an .Rmd file.\nOnce this is done one has to define the questions to be included (the pool) and set the details for the selection process:\nsol &lt;- exams2pdf(myexam, \nn = 2, \nnsamp = 5, \ndir = odir, \n template = c(\"my_exam\", \"solution\"), \n encoding = 'UTF-8',\n header = list(Date = \"10.06.2016\")\n)\nThis code would give me 2 exams with a sample of 5 questions out of each block of questions.\nPretty awesome (after some setup work).\nThanks Achim et al.¬†!!"
  },
  {
    "objectID": "posts/2015-11-10-new-paper-on-pychodiagnosis-and-eye-tracking.html",
    "href": "posts/2015-11-10-new-paper-on-pychodiagnosis-and-eye-tracking.html",
    "title": "New Paper on pychodiagnosis and eye-tracking",
    "section": "",
    "text": "Cilia Witteman and Nanon Spaanjaars (my dutch connection) worked together on a piece on whether psychodiagnosticians improve over time (they don‚Äôt) in their ability to classify symptoms to DSM categories. This turned out to be a pretty cool paper combining eye-tracking data with a practical, and hopefully,¬†relevant question.\nSchulte-Mecklenbeck, M., Spaanjaars, N.L., & Witteman, C.L.M. (in press). The (in)visibility of psychodiagnosticians‚Äô expertise. Journal of Behavioral Decision Making.\nThis study investigates decision making in mental health care. Specifically, it compares the diagnostic decision outcomes (i.e., the quality of diagnoses) and the diagnostic decision process (i.e., pre-decisional information acquisition patterns) of novice and experienced clinical psychologists. Participants‚Äô eye movements were recorded while they completed diagnostic tasks, classifying mental disorders. In line with previous research, our findings indicate that diagnosticians‚Äô performance is not related to their clinical experience. Eye-tracking data pro- vide corroborative evidence for this result from the process perspective: experience does not predict changes in cue inspection patterns. For future research into expertise in this domain, it is advisable to track individual differences between clinicians rather than study differences on the group level."
  },
  {
    "objectID": "posts/2011-08-02-why-anybody-should-learnuse-r.html",
    "href": "posts/2011-08-02-why-anybody-should-learnuse-r.html",
    "title": "Why anybody should learn/use R ‚Ä¶",
    "section": "",
    "text": "I had a discussion the other day on the re-appearing topic why one should learn R ‚Ä¶\nI took the list below from the R-Bloggers¬†which argues why grad students should learn R:\n\nR is free, and lets grad students escape the burdens of commercial license costs.\nR has really good online documentation; and the community is unparalleled.¬†¬†\nThe command-line interface is perfect for learning by doing.¬†\nR is on the cutting edge, and expanding rapidly.\nThe R programming language is intuitive.¬†¬†\nR creates stunning visuals.¬†\nR and LaTeX work together ‚Äî seamlessly.¬†\nR is used by¬†practitioners¬†in a plethora of academic disciplines.¬†\nR makes you think.¬†¬†\nThere‚Äôs always more than one way to accomplish something.\n\nThis is a great list ‚Äì I would add that from the perspective of an university it makes sense to save a lot of money in not having to buy licenses. And reproducability is great with R because the code is always written in a text-file and not bound by software versions (as in other three or four letter (feel free to combine from: [A, P, S]) packages)."
  },
  {
    "objectID": "posts/2009-07-13-flashlight-paper-draft.html",
    "href": "posts/2009-07-13-flashlight-paper-draft.html",
    "title": "Flashlight paper draft",
    "section": "",
    "text": "We submitted our Flashlight paper today. Find a draft at the address below:\nSchulte-Mecklenbeck, Michael , Murphy, Ryan O. and Hutzler, Florian,Flashlight ‚Äì an Online Eye-Tracking Tool(July 13, 2009). Available at SSRN: http://ssrn.com/abstract=1433225\n\nThe software will be uploaded to the project page http://vlab.ethz.ch/vLab_Decision_Theory_and_Behavioral_Game_Theory/Flashlight.html.\nHave fun playing with it!"
  },
  {
    "objectID": "posts/2018-12-10-bernr-meetup.html",
    "href": "posts/2018-12-10-bernr-meetup.html",
    "title": "BernR Meetup",
    "section": "",
    "text": "Today (Dec 10th 2018) we will meet for the first BernR Meetup (https://www.meetup.com/Bern-R/) ‚Äì hope to learn new things and get to know cool R people. More to follow soon .."
  },
  {
    "objectID": "posts/2017-04-12-growing-up-to-be-old.html",
    "href": "posts/2017-04-12-growing-up-to-be-old.html",
    "title": "Growing up to be old",
    "section": "",
    "text": "Some papers have somewhat weird starting points ‚Äì this one had an awesome starting point ‚Äì Lake Louise (Canada):\n\n\n\nFigure¬†1: BANF\n\n\nIn a little suite we (Joe Johnson, Ulf B√∂ckenholt, Dan Goldstein, Jay Russo, Nikki Sullivan, Martijn Willemsen) sat down¬†during a conference called the ‚ÄòChoice Symposium‚Äò and started working on an overview paper about the history and current status of¬†different process tracing methods. One central result (why can‚Äôt all papers be like that) is the figure below where we try to locate many process tracing methods¬†on the two dimensions: temporal resolution and distortion risk (i.e., how fast can a method measure a process and how destructive is this measurement).\nSchulte-Mecklenbeck, M., Johnson, J.G., B√∂ckenholt, U., Goldstein, D., Russo, J., Sullivan, N., & ¬†Willemsen, M. (2017). Process tracing methods in decision making: On growing up in the 70ties. Current Directions in Psychological Science."
  },
  {
    "objectID": "posts/2009-10-24-lattice-versus-ggplot2.html",
    "href": "posts/2009-10-24-lattice-versus-ggplot2.html",
    "title": "Lattice versus ggplot2",
    "section": "",
    "text": "I really liked Lattice for generating graphs in R until I saw what ggplot2 can do ‚Ä¶\nOne of the big differences between the two is the theory on which ggplot2 is based upon. There are clear modular building blocks that can be applied in a consistent manner on any graph generated. Both packages are extremely versatile but at the end of the day I think ggplot2 provides a clearer structure and hence more flexibility ‚Ä¶\nHadley Wickham (the author of ggplot2) has a book out on ggplot2 at Springer. Some sample chapters can be downloaded from his webpage.\nThe learningR people have a long series of posts where they provide ggplot2 code for nearly all the graphs in the Lattice book ‚Ä¶ worth taking a look at!\nHere is an in depth"
  },
  {
    "objectID": "posts/2012-10-13-apply-in-all-its-variations.html",
    "href": "posts/2012-10-13-apply-in-all-its-variations.html",
    "title": "*apply in all its variations ‚Ä¶",
    "section": "",
    "text": "Here¬†is an excellent stackoverflow post on how apply in all its variations can be used.\nOne of the followups points at plyr (from demi-R-god Hadley Wickham) which provides a consistent naming convention for all the apply variations. I like plyr a lot, because like ggplot, it is easy to grasp and relatively intuitive to find an answer to even tricky problems.\nHere is the translation from *apply to plyr ‚Ä¶\nBase function ¬† Input ¬† Output ¬† plyr function \n---------------------------------------\naggregate ¬† ¬† ¬† ¬†d ¬† ¬† ¬† d ¬† ¬† ¬† ddply + colwise \napply ¬† ¬† ¬† ¬† ¬† ¬†a ¬† ¬† ¬† a/l ¬† ¬† aaply / alply \nby ¬† ¬† ¬† ¬† ¬† ¬† ¬† d ¬† ¬† ¬† l ¬† ¬† ¬† dlply \nlapply ¬† ¬† ¬† ¬† ¬† l ¬† ¬† ¬† l ¬† ¬† ¬† llply ¬†\nmapply ¬† ¬† ¬† ¬† ¬† a ¬† ¬† ¬† a/l ¬† ¬† maply / mlply \nreplicate ¬† ¬† ¬† ¬†r ¬† ¬† ¬† a/l ¬† ¬† raply / rlply \nsapply ¬† ¬† ¬† ¬† ¬† l ¬† ¬† ¬† a ¬† ¬† ¬† laply"
  },
  {
    "objectID": "posts/2012-05-24-religion-and-of-babies.html",
    "href": "posts/2012-05-24-religion-and-of-babies.html",
    "title": "Religion and # of babies ‚Ä¶",
    "section": "",
    "text": "Religion seems to have little influence on the # of babies per woman ‚Äì who might have thought ‚Ä¶\nHere is another excellent talk of Hans R√∂sling on the topic."
  },
  {
    "objectID": "posts/2011-07-15-awesome-visualization-tool-for-r.html",
    "href": "posts/2011-07-15-awesome-visualization-tool-for-r.html",
    "title": "awesome visualization tool for R",
    "section": "",
    "text": "http://cran.r-project.org/web/packages/googleVis/ enables chart generation similar to the ones instroduced by Hans Roesling¬†in his TED talk¬†on poverty ‚Äì extremly cool üôÇ"
  },
  {
    "objectID": "posts/2015-03-11-about-illusions.html",
    "href": "posts/2015-03-11-about-illusions.html",
    "title": "about illusions",
    "section": "",
    "text": "Andrew Gelman¬†talked about a really old paper I did together with Anton K√ºhberger ages ago. It was actually the first paper / ‚Äòreal‚Äô scientific project I was involved in.\nIt¬†generated quite the buzz over its 20 year lifespan and was cited a whopping 13 times (stats look good without y-axis) ‚Ä¶\n\n¬†\n¬†\n¬†\nGoing back to it, I was¬†happy to see that we already talked about replication (and were very reluctant to push the button harder ‚Äì as we would have not been able to get through the reviews, I guess) ‚Ä¶ Things have changed."
  },
  {
    "objectID": "posts/2011-04-10-sustainable-research.html",
    "href": "posts/2011-04-10-sustainable-research.html",
    "title": "Sustainable research",
    "section": "",
    "text": "A sister blog with interesting contributions on what open source software researcher can use to work sustainable ‚Ä¶\n\n\nhttp://www.surefoss.org/"
  },
  {
    "objectID": "posts/2023-05-20-default/index.html",
    "href": "posts/2023-05-20-default/index.html",
    "title": "Replication: Default effect",
    "section": "",
    "text": "Johnson, E. J., Bellman, S., & Lohse, G. L. (2002). Defaults, framing and privacy: Why opting in-opting out."
  },
  {
    "objectID": "posts/2023-05-20-default/index.html#abstract",
    "href": "posts/2023-05-20-default/index.html#abstract",
    "title": "Replication: Default effect",
    "section": "Abstract",
    "text": "Abstract\nDifferences in opt-in and opt-out responses are an important element of the current public debate concerning on-line privacy and more generally for permission marketing. We explored the issue empirically. Using two on-line experiments we show that the default has a major role in determining revealed preferences for further contact with a Web site. We then explore the origins of these differences showing that both framing and defaults have separate and additive effects in affecting the construction of preferences."
  },
  {
    "objectID": "posts/2023-05-20-default/index.html#experiment-1",
    "href": "posts/2023-05-20-default/index.html#experiment-1",
    "title": "Replication: Default effect",
    "section": "Experiment 1",
    "text": "Experiment 1\nJohnson et al.¬†(2002) used four questions to evaluate the presence of a default (no default in question 1 and 2 (listed both as 1 in the original) and two questions (!3 and Q4) with a pre-set default selection) and crossed these with framing (positive (Q1, Q3) and negative (Q2, Q4)). Putting the results in a rank order (see Figure¬†1) we are getting the highest participation rate in Q2 (no default, negative) followed by Q3 (default, positive), Q4 (default, negative) and the lowest in Q1 (no default, positive)) - so we are looking for the sequence 2341.\n\n\n\nFigure¬†1: Items Johnson et al.¬†Exp 1\n\n\nOur results show the pattern 3214 as can be seen in Figure¬†2. Noteworthy seems that Q4 (default, negative) was de-selected in most cases which is surprising and different to the original.\n\n\n\n\n\nFigure¬†2: Results Replication Experiment 1\n\n\n\n\nRunning the logistic regression predicting choice with framing X default (see Table¬†1) shows a framing effect as well as an effect for the default condition (of course in the opposite direction). The interaction term is also significant.\n\n\n\n\nTable¬†1: Logistic regression for Experiment 1\n\n\n\n\n\n\n\n\n¬†\nchoice\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n0.08\n0.01¬†‚Äì¬†0.28\n0.001\n\n\nFraming [FP]\n24.00\n3.78¬†‚Äì¬†232.46\n0.002\n\n\nDefault condition [ND]\n9.82\n2.11¬†‚Äì¬†71.95\n0.008\n\n\nFraming [FP] √ó Default\ncondition [ND]\n0.02\n0.00¬†‚Äì¬†0.20\n0.001\n\n\nObservations\n76\n\n\nR2 Tjur\n0.184"
  },
  {
    "objectID": "posts/2023-05-20-default/index.html#experiment-2",
    "href": "posts/2023-05-20-default/index.html#experiment-2",
    "title": "Replication: Default effect",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn experiment 2 Johnson et al.¬†(2002) modify the questions (see Figure¬†3) in adding a Yes/No option (instead the simple tick box in Experiment 1) and an empty version where no boxes are ticked. Inspecting the results the rank order is now 651243.\n\n\n\n\n\nItems Johnson et al.¬†Exp 2\n\n\nFigure¬†3: ?(caption)\n\n\nThere is a clear framing effect and a step wise default effect (see Figure¬†4).\n\n\n\nFigure¬†4: Results Johnson et al.¬†Exp 2\n\n\nThere several differences inspecting our results. Our rank ordered results give us the pattern: 612534. Remarkable are also the much lower participation rates (below 50%) for Not participate and No Default in both framing conditions. Only Participate in the positive frame is above 75% (see Figure¬†5).\n\n\n\n\n\nFigure¬†5: Results Replication Experiment 2\n\n\n\n\nRunning the logistic regression predicting choice with framing X default X answer option (see Table¬†2) shows a significant framing effect as well as an effect for the the presence of an answer option (Y/N). The two-way interaction terms for framing and default is significant. There is also a three-way-interaction of framing, default and answer option mainly driven by the difference between participate and no-default.\n\n\n\n\nTable¬†2: Logistic regression for Experiment 2\n\n\n\n\n\n\n\n\n¬†\nchoice\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n1.20\n0.36¬†‚Äì¬†4.16\n0.763\n\n\nFraming [FP]\n8.33\n1.42¬†‚Äì¬†69.92\n0.027\n\n\nDefault condition [DP]\n0.97\n0.19¬†‚Äì¬†4.96\n0.973\n\n\nDefault condition [ND]\n1.25\n0.25¬†‚Äì¬†6.18\n0.781\n\n\nAnswer Option [Y]\n0.08\n0.00¬†‚Äì¬†0.67\n0.040\n\n\nFraming [FP] √ó Default\ncondition [DP]\n0.03\n0.00¬†‚Äì¬†0.29\n0.004\n\n\nFraming [FP] √ó Default\ncondition [ND]\n0.43\n0.03¬†‚Äì¬†5.01\n0.499\n\n\nFraming [FP] √ó Answer\nOption [Y]\n0.12\n0.01¬†‚Äì¬†4.10\n0.186\n\n\nDefault condition [DP] √ó\nAnswer Option [Y]\n8.82\n0.63¬†‚Äì¬†255.72\n0.132\n\n\nDefault condition [ND] √ó\nAnswer Option [Y]\n5.33\n0.40¬†‚Äì¬†149.91\n0.239\n\n\n(Framing [FP] √ó Default\ncondition [DP]) √ó Answer\nOption [Y]\n181.48\n2.90¬†‚Äì¬†9602.29\n0.009\n\n\n(Framing [FP] √ó Default\ncondition [ND]) √ó Answer\nOption [Y]\n0.66\n0.01¬†‚Äì¬†31.07\n0.833\n\n\nObservations\n190\n\n\nR2 Tjur\n0.351"
  },
  {
    "objectID": "posts/2011-04-14-resinstalling-packages-in-r-after-update.html",
    "href": "posts/2011-04-14-resinstalling-packages-in-r-after-update.html",
    "title": "resinstalling packages in R after update",
    "section": "",
    "text": "This is old - by should still work :) - comment on how to do this in 2019 below ‚Ä¶\nThe new version 2.13.0 of R¬†has just been released and with the update comes the pain of re-installing all the packages from the old installation on the new one.\nStackoverflow¬†to the rescue! This posting¬†provides a simple two step process of first writing a list of packages into a file on the disk in the old version, installing the new version and then¬†comparing the¬†exported list to the currently installed packages in the new version with setdiff. I just went through the process and have to say that it is deadeasy! Below the¬†code:\n¬† 1. run in the old version of R\npackages &lt;- installed.packages()[,\"Package\"]¬†\nsave(packages, file=\"Rpackages\")¬†`\n\nInstall new R version\nrun in the new version\n\nload(\"Rpackages\")\nfor (p in setdiff(packages, installed.packages()[,\"Package\"]))\ninstall.packages(p)\nAt the end of the day this is outdated - with the new RStudio version you can simply add your packages with library() and then simply click on install packages in the RStudio interface."
  },
  {
    "objectID": "posts/2011-03-01-blog-attack.html",
    "href": "posts/2011-03-01-blog-attack.html",
    "title": "Blog-attack",
    "section": "",
    "text": "Here is what Andrew Gelman writes about an interview Colin Camerer gave on Spousomonics ‚Ä¶ Looking forward to Camerer‚Äôs reply ‚Ä¶"
  },
  {
    "objectID": "posts/2010-04-16-r-goes-cloud.html",
    "href": "posts/2010-04-16-r-goes-cloud.html",
    "title": "R goes cloud",
    "section": "",
    "text": "Jeroen Ooms did for R what Google did for editing documents online. He created several software packages that help running R with a nice frontend over the Internet.\nI first learned about Jeroen‚Äôs website through his implementation of ggplot2 ‚Äì this page is useful to generate graphs with the powerful ggplot2 package without R knowledge, however it is even more helpful to learn ggplot2 code with the View-code panel function which displays the underlying R code. If you are into random effect models another package connected to lme4 will guide you step by step through model building.\nI think this is a great step forward for R and cloud computing!"
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html",
    "title": "Replication: In search of homo economicus",
    "section": "",
    "text": "Understanding the role of emotion in forming preferences is critical in helping firms choose effective marketing strategies and consumers make appropriate consumption decisions. In five experiments, participants made a set of binary product choices under conditions designed to induce different degrees of emotional decision processing. The results consistently indicate that greater reliance on emotional reactions during decision making is associated with greater preference consistency and less cognitive noise. Additionally, the results of a meta-analytical study based on data from all five experiments further show that products that elicit a stronger emotional response are more likely to yield consistent preferences."
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#abstract",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#abstract",
    "title": "Replication: In search of homo economicus",
    "section": "",
    "text": "Understanding the role of emotion in forming preferences is critical in helping firms choose effective marketing strategies and consumers make appropriate consumption decisions. In five experiments, participants made a set of binary product choices under conditions designed to induce different degrees of emotional decision processing. The results consistently indicate that greater reliance on emotional reactions during decision making is associated with greater preference consistency and less cognitive noise. Additionally, the results of a meta-analytical study based on data from all five experiments further show that products that elicit a stronger emotional response are more likely to yield consistent preferences."
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#original-work",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#original-work",
    "title": "Replication: In search of homo economicus",
    "section": "Original work",
    "text": "Original work\nLee, L., Amir, O., & Ariely, D. (2009). In search of homo economicus: Cognitive noise and the role of emotion in preference consistency."
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#demographics",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#demographics",
    "title": "Replication: In search of homo economicus",
    "section": "Demographics",
    "text": "Demographics\nWe sampled 150 participants (44% female, 1% did not specify gender), with an average age M = 44.4, SD = 11.3 years. Participants were recruited via Amazon Mechanical Turk (AMT) and paid a flat fee of USD 7.25/hour for completing the task."
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#method",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#method",
    "title": "Replication: In search of homo economicus",
    "section": "Method",
    "text": "Method\nWe replicated Experiment 1a PICTURES VERSUS NAMES and Experiment 2 COLOR VERSUS BLACK- AND-WHITE PICTURES. Participants were randomly assigned to one of the two experiments, giving us 75 participants per experiment and ~37 per condition.\nParticipants first inspected information about 10 consumer items (see example below) including the name of the product, a picture and a short description. Participants were asked to study the products as long as they wanted.\nOut of the 10 products we generated a list of 45 pairwise choice with all possible combinations of products, where\n\\(combinations = {P*(P-1) \\over 2}\\)\nhence, 45 combinations.\nEach participant then went through these 45 choices indicating her preference between the shown product. As indicated above, we replicated two experiments (1a and 2) with the following variations. In 1a the choice list consisted out of product pictures only (picture) or product names only (names); in 2 pictures were either in color (color) or black and white (b&w). Both manipulations represent different degrees of emotional and cognitive approaches to decision making."
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#analysis",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#analysis",
    "title": "Replication: In search of homo economicus",
    "section": "Analysis",
    "text": "Analysis\nAs the dependent measure we calculated (in)consistency in choice patterns."
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#experiment-1a-picture-v-names",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#experiment-1a-picture-v-names",
    "title": "Replication: In search of homo economicus",
    "section": "Experiment 1a picture v names",
    "text": "Experiment 1a picture v names\nLee and colleagues report significant fewer transitivity violations in the picture condition (M = 2.7, SD = 4.7) than in the names condition (M = 4.6, SD = 6.3, t(532) = 4.08, p &lt; .001). Using the formula for Cohen‚Äôs d we want to calculate:\n\\(d = {M_{2} - M_{1} \\over SD_{pooled}}\\)\nwhere\n\\(SD_{pooled} = {\\sqrt{SD_{1}^2 + SD_{2}^2} \\over 2}\\)\n\nsd_pooled &lt;- sqrt((4.7^2 + 6.3^2)/2)\nd &lt;- (4.6 - 2.7) / sd_pooled\n\nSo, we get a Cohen‚Äôs d = 0.34, which constitutes a small effect according to Cohen, 1992. Lets see how our own replication does for Experiment 1a.\nOur 150 participants each made 45 choices, so we are expecting a tibble with 150*45 = 6750 rows. Lets see:\n\n# generate data frame for choices\nLee_Choice_Coded &lt;- \nLee_Choice %&gt;%\n  left_join(Lee_Items) %&gt;%\n  mutate(combi1 = str_extract(combi1, pattern = \"(\\\\d)+\"),\n         combi2 = str_extract(combi2, pattern = \"(\\\\d)+\"),\n         combination = paste0(combi1, combi2))\n\n# subset relevant variables data frame \nLee_Choice_Select &lt;- \n   Lee_Choice_Coded %&gt;%\n   select(Experiment, condition, ID, combination, coded_choice)\nLee_Choice_Select\n\n# A tibble: 6,750 √ó 5\n   Experiment condition ID       combination coded_choice\n   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;              &lt;int&gt;\n 1 Lee2       pictures  8Y3uw8Wz 12                     2\n 2 Lee2       pictures  8Y3uw8Wz 13                     3\n 3 Lee2       pictures  8Y3uw8Wz 14                     4\n 4 Lee2       pictures  8Y3uw8Wz 15                     5\n 5 Lee2       pictures  8Y3uw8Wz 16                     1\n 6 Lee2       pictures  8Y3uw8Wz 17                     7\n 7 Lee2       pictures  8Y3uw8Wz 18                     8\n 8 Lee2       pictures  8Y3uw8Wz 19                     1\n 9 Lee2       pictures  8Y3uw8Wz 110                    1\n10 Lee2       pictures  8Y3uw8Wz 23                     3\n# ‚Ä¶ with 6,740 more rows\n\n\nFor both experiments (1a and 2) we have for each condition (condition) and participant (ID) the relevant item comparison (combination) and 45 responses (coded_choice, a numeric value for each possible answer, as described in Lee_Items). Note that we updated the items to something more relevant these days.\n \n\nCheck for transitivity\nLee et al.¬†write on p.¬†176 ‚ÄúFor simplicity in reporting the results, we focus on violations in the form of three-way preference cycles (e.g., \\(p_{x} ‚â• p_{y}, p_{y} ‚â• p_{z}\\) and \\(p_{z} ‚â• p_{x}\\))‚Äù\nSo, we will do the same :) - first we setup a tibble with all possible triplets and then identify (in)transitivity for choices on each product pair and then pair-triplets.\ngtools provides functions to ‚ÄúEnumerate the Combinations or Permutations of the Elements of a Vector‚Äù - cool - exactly what we need ‚Ä¶ The function takes two arguments n = the size of the source vector, in our case 10 products, and r = size of the target vector, our triplets. This gives us a 720x3 matrix (only the first 5 rows displayed here).\n\n# generate combinations\ncomb &lt;- permutations(n = 10, r = 3)\nhead(comb, 5)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    1    2    4\n[3,]    1    2    5\n[4,]    1    2    6\n[5,]    1    2    7\n\n\nWe transfer the matrix into a tibble and paste together all combinations of products - so there are three pairs that identify the three combinations of products. The new variable defines all transitive choices for all possible permutations of choice pairs.\n\n# generate space with all possible choice combinations\nspace &lt;- \nas_tibble(comb) %&gt;%\n  mutate(pair1 = paste0(V1,V2),\n         pair2 = paste0(V2,V3),\n         pair3 = paste0(V1,V3),\n         transitive = paste0(V1,V2,V1)) %&gt;%\n  select(-starts_with('V'))\nspace\n\n# A tibble: 720 √ó 4\n   pair1 pair2 pair3 transitive\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     \n 1 12    23    13    121       \n 2 12    24    14    121       \n 3 12    25    15    121       \n 4 12    26    16    121       \n 5 12    27    17    121       \n 6 12    28    18    121       \n 7 12    29    19    121       \n 8 12    210   110   121       \n 9 13    32    12    131       \n10 13    34    14    131       \n# ‚Ä¶ with 710 more rows\n\n\nNow for some joining. We want to join the actual choices of each participant back into the matrix we generated above, with all the permutations of my 10 items. This is a little step by step operation, cause we want to join based on three different variables (pairs 1, 2 and 3). The first join is straight forward, for every row in pair 1 match the choice (and all other information about participants, experiment etc) from Lee_Choice_Coded, then rename the coded_choice variable and go on and match on pair2 now - note that we add ID and condition (although ID should be enough?), rename again and then match based on pair3.\n\n\n# A tibble: 54,360 √ó 10\n   pair1 pair2 pair3 transitive Experiment condi‚Ä¶¬π ID    coded‚Ä¶¬≤ coded‚Ä¶¬≥ coded‚Ä¶‚Å¥\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1 12    23    13    121        Lee2       pictur‚Ä¶ 8Y3u‚Ä¶       2       3       3\n 2 12    23    13    121        Lee4       B&W     D9JS‚Ä¶       2       2       3\n 3 12    23    13    121        Lee4       B&W     3iTJ‚Ä¶       2       2       3\n 4 12    23    13    121        Lee2       pictur‚Ä¶ 47Z1‚Ä¶       2       2       3\n 5 12    23    13    121        Lee3       color   KLMD‚Ä¶       2       3       3\n 6 12    23    13    121        L1a        names   ZVQF‚Ä¶       2       2       3\n 7 12    23    13    121        Lee4       B&W     u0L1‚Ä¶       2       2       3\n 8 12    23    13    121        Lee2       pictur‚Ä¶ nOff‚Ä¶       2       2       1\n 9 12    23    13    121        Lee3       color   Hstl‚Ä¶       2       2       3\n10 12    23    13    121        L1a        names   zEAh‚Ä¶       2       3       3\n# ‚Ä¶ with 54,350 more rows, and abbreviated variable names ¬π‚Äãcondition,\n#   ¬≤‚Äãcoded_choice1, ¬≥‚Äãcoded_choice2, ‚Å¥‚Äãcoded_choice3\n\n\nSo, that looks about right - what is left to do is to paste together the actual choices and then test these choice triples coded_choice_triple against the transitive variable we generated above - which gives us a logical vector called test.\n\n\n# A tibble: 54,360 √ó 6\n   transitive Experiment condition ID       choice_triple test \n   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;         &lt;lgl&gt;\n 1 121        Lee2       pictures  8Y3uw8Wz 233           FALSE\n 2 121        Lee4       B&W       D9JSme3C 223           FALSE\n 3 121        Lee4       B&W       3iTJmoTV 223           FALSE\n 4 121        Lee2       pictures  47Z1QWBC 223           FALSE\n 5 121        Lee3       color     KLMDkXi9 233           FALSE\n 6 121        L1a        names     ZVQFirwo 223           FALSE\n 7 121        Lee4       B&W       u0L1YrCo 223           FALSE\n 8 121        Lee2       pictures  nOffEONb 221           FALSE\n 9 121        Lee3       color     Hstldczk 223           FALSE\n10 121        L1a        names     zEAhW9Wh 233           FALSE\n# ‚Ä¶ with 54,350 more rows\n\n\n\n\n# A tibble: 5 √ó 4\n# Groups:   Experiment [5]\n  Experiment condition av_intrans sd_intrans\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 L1a        names           53.1       24.2\n2 Lee2       pictures        49.9       28.0\n3 Lee3       color           47.0       23.5\n4 Lee4       B&W             46.7       29.3\n5 &lt;NA&gt;       &lt;NA&gt;           100         NA"
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#test-experiment-1",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#test-experiment-1",
    "title": "Replication: In search of homo economicus",
    "section": "Test Experiment 1",
    "text": "Test Experiment 1\nWe run two t-tests. For Experiment 1: names v. pictures.\n\n\n\n    Welch Two Sample t-test\n\ndata:  intransitive by condition\nt = 0.52209, df = 72.026, p-value = 0.6032\nalternative hypothesis: true difference in means between group names and group pictures is not equal to 0\n95 percent confidence interval:\n -3.996787  6.833203\nsample estimates:\n   mean in group names mean in group pictures \n              23.89189               22.47368 \n\n\n\nCohen's d\n\nd estimate: 0.1203499 (negligible)\n95 percent confidence interval:\n     lower      upper \n-0.3403704  0.5810702"
  },
  {
    "objectID": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#test-experiment-2",
    "href": "posts/2022-09-24-replication-in-search-of-homo-oeconomicus/index.html#test-experiment-2",
    "title": "Replication: In search of homo economicus",
    "section": "Test Experiment 2",
    "text": "Test Experiment 2\nA second one for Experiment 2: color v. B&W\n\n\n\n    Welch Two Sample t-test\n\ndata:  intransitive by condition\nt = -0.039508, df = 70.46, p-value = 0.9686\nalternative hypothesis: true difference in means between group B&W and group color is not equal to 0\n95 percent confidence interval:\n -5.601652  5.384013\nsample estimates:\n  mean in group B&W mean in group color \n           21.02632            21.13514 \n\n\n\nCohen's d\n\nd estimate: -0.009098058 (negligible)\n95 percent confidence interval:\n     lower      upper \n-0.4694043  0.4512082 \n\n\nIn the two ways we look at the two experiments - t-test significance and Cohen‚Äôs d - well, there is not a lot going on in our replication - we get non-significant results for both Tests and negligible effect sizes."
  },
  {
    "objectID": "posts/2016-09-20-everything-you-believe-in-is-wrong-or-is-it-simply-terrorism.html",
    "href": "posts/2016-09-20-everything-you-believe-in-is-wrong-or-is-it-simply-terrorism.html",
    "title": "Everything you believe in is wrong ‚Äì or is it simply terrorism?",
    "section": "",
    "text": "The replication crisis has many interesting effects on how people (and scientists) think about Psychology (and, of course, other fields) ‚Ä¶ Here is a nice summary of effects that are hard to replicate. Among them ‚Äòclassics‚Äô like the power pose or the big brother eyes.\nA lot is happening because of these new insights in terms of research (e.g., replication studies) and communication (e.g., Fritz Strack on Facebook).\nAnd then this: Susan Fiske in an upcoming piece in the APS Observer ‚Ä¶ I am really struggling with this rhetoric ‚Äì Daniel Lakens to the rescue üôÇ\nAh ‚Äì and of course Gelman."
  },
  {
    "objectID": "posts/2016-01-10-three-weeks-without-email.html",
    "href": "posts/2016-01-10-three-weeks-without-email.html",
    "title": "Three weeks without email",
    "section": "",
    "text": "I spend a lot of time writing and answering email. Email is, according to¬†timing, the third longest activity on my computer (although I am using three¬†computers and can check this only on one of them ‚Äì #timing please let us link computers for an overall analysis) ‚Ä¶ anyway ‚Äì back to no email ‚Äì as a holiday treat I decided to shut down all my email accounts 5 days before Dec.¬†24th and promised myself not to touch them until Jan.¬†11th. It turns out that I will come short one day of this plan. Nevertheless, I am quite happy with the result and the positive effects of this email absence. Needless to say that reading emails during vacation brings you back into a working mood (or never lets you out of it), not reading email had positive side effects before (I did this twice in the last 20 years of ‚Äòdoing‚Äô emails). Many issues that come during such a break often solve themselves without¬†intervention or can be solved quickly within a few hours after being back in the email world.\nWell, I will turn on my email accounts now and see¬†how much work has piled up ‚Ä¶ BRB.\nSo, 380 emails later ‚Äì a paper submitted by a co-author, a rejection for a previous submission, a talk accepted, a chapter revised by a co-author ‚Äì the best part of all this is that dealing with a ton of mails is a very quick thing, with a relatively low threshold for simply deleting out of date emails or replying quickly to urgent matters. What‚Äôs left are some longer replies I will do now ‚Ä¶\nHappy New Year!"
  },
  {
    "objectID": "posts/2011-08-18-how-decisions-deplete-and-breaks-help.html",
    "href": "posts/2011-08-18-how-decisions-deplete-and-breaks-help.html",
    "title": "how decisions deplete and breaks help",
    "section": "",
    "text": "The New York Times published a nice overview of the work on decision making and¬†ego depletion¬†(often ego depletion is used as a synonym with resource depletion, which is somewhat confusing because of the use of the later in economy to described the situation when raw materials are exhausted in a region).\nA new paper from Jonathan Levav (now at Standford ‚Äì congrats!) is prominently featured in the above article. Levav et al analysed rulings in court cases and link them to the time when these rulings were made during the day. They showed that after breaks (lunches) the probability of getting probation dramatically increased, a results that blends nicely into the ego depletion idea which states that self-control is a limited resource that is depleted by decision (rulings) and can be restored by, e.g., rest."
  },
  {
    "objectID": "posts/2023-10-28-kinder/index.html",
    "href": "posts/2023-10-28-kinder/index.html",
    "title": "Kinderuni",
    "section": "",
    "text": "Sometimes teaching turns out to be very different from the usual lecture on a Wednesday. I was asked (well I signed up) to teach in the Kinderuni (children‚Äôs university) at my home institution, the University of Bern. This was part of program where children between the ages of 8-12 are invited to participate in a lecture (Friday) and then in a seminar (Saturday). In what follows I will mainly talk about the lecture."
  },
  {
    "objectID": "posts/2023-10-28-kinder/index.html#anchoring",
    "href": "posts/2023-10-28-kinder/index.html#anchoring",
    "title": "Kinderuni",
    "section": "Anchoring",
    "text": "Anchoring\nI talked about decision making and experiments as a methodological approach. My aim was to have many examples and run experiments with them in class. 75 children showed up on Friday for the lecture. Having a mobile makes running in-class experiments really easy - but I could not rely on that - hence back to paper and pencil. We started off with an anchoring task, the classic: multiply the following numbers 1*2*3*4*5*6*7*8*9 for one group and the reverse for the second seemed a little to hard for 8-years old so I used addition instead 1+2+3+4+5+6+7+8+9. This worked reasonably well (a brave helper typed in the numbers, while I worked through another example with them) and we could demonstrate quite a substantial anchoring effect (400 v 1400) which was explained back to me, by one of the kids in great detail - pretty cool."
  },
  {
    "objectID": "posts/2023-10-28-kinder/index.html#wisdom-of-the-crowds",
    "href": "posts/2023-10-28-kinder/index.html#wisdom-of-the-crowds",
    "title": "Kinderuni",
    "section": "Wisdom of the crowds",
    "text": "Wisdom of the crowds\nAs mentioned above, while a lot of typing was going on I let them estimate three quantities: I had 1) a big glass vase with ping pong balls. 2) a picture of many Legos and 3) I let them estimate my weight.\n\n\n\nFigure¬†2: Estimations\n\n\nThere were 44 balls, 88 Legos and my own weight (on the morning of the lecture) 76 kg. The average of the whole group was off by 1 ball (pretty good!), 4 Legos and 5 kg (I just look too slim I guess) - we talked a bit about individual choices and the averages - again tricky for the little ones, pretty ok for the older."
  },
  {
    "objectID": "posts/2023-10-28-kinder/index.html#risk",
    "href": "posts/2023-10-28-kinder/index.html#risk",
    "title": "Kinderuni",
    "section": "Risk",
    "text": "Risk\nWe moved on to decision making without risk (food choice) and with risk. Here I tried to show them base rate neglect with several picks of a certain gummybear color (green or red) from different distributions.\n We started out with an easy task - do you prefer A or B if you want to draw a red gummybear?\n After some further easy tasks - something more tricky - after bringing these in order, it became clear that the two distributions were the same - so same probabilities for A and B.\n\n\n\nFigure¬†3: Equal probabiliites\n\n\nFinally the real tricky one:\n\n\n\nFigure¬†4: Tricky\n\n\nJust going with the number of red ones (if you want a red) is misleading ‚Ä¶ we do a bit of maths and decide that A is advantageous. Clearly the 8-year olds had troubles with this - but the older ones totally got my point and explained that A simply has the larger probability of a red gummybear - hence this one is preferable - again - pretty cool.\n\n\n\nFigure¬†5: Risk"
  },
  {
    "objectID": "posts/2023-10-28-kinder/index.html#game-theory",
    "href": "posts/2023-10-28-kinder/index.html#game-theory",
    "title": "Kinderuni",
    "section": "Game theory",
    "text": "Game theory\nFinally - and definitly most chaotic of all - we played two rounds of game theory. First a dictator game, imagining one of the other kids as their partner. Then I paired them up and let them play an Ultimatum game in pairs with (Sugus)[https://en.wikipedia.org/wiki/Sugus] sweets.\n\n\n\nFigure¬†6: Dictator game\n\n\n\n\n\nFigure¬†7: Ultimatum game\n\n\nBoth rounds were pretty wild but we ended up in a nice discussion on fairness - a fair offer (3:3 when 6 Sugus were distributed) was clearly identified and other cases mentioned. Some interactions ended in interesting distributions like 6:0 cause one of the two kids did not like Sugus - so well coordinated :)."
  },
  {
    "objectID": "posts/2023-10-28-kinder/index.html#take-aways",
    "href": "posts/2023-10-28-kinder/index.html#take-aways",
    "title": "Kinderuni",
    "section": "Take Aways",
    "text": "Take Aways\n\nI was way off how long things take - favorite example: I told them to form pairs of two for the ultimatum game ‚Ä¶ it took us about 10 minutes to achive this, planned about 2 :)\nresponses in paper and pencil are tricky to live analyse - I normally run FormR as a frontend and a R script in the background that analyse/visualize data automatically - easy, once setup. Here we did two things - entering values from the response papers into Google Sheets and sticking dots onto a flipchart to generate a distribution graph - both works but both takes a lot of time (even with 3 people in the room)\nKids (at least in this class) are super responsive and engaged - I am used to getting not a lot of feedback/questions from my students - with the kids 7-10 hands are up for every questions - great, but again a time issue if you want to address the questions properly"
  },
  {
    "objectID": "posts/2023-10-28-kinder/index.html#slides",
    "href": "posts/2023-10-28-kinder/index.html#slides",
    "title": "Kinderuni",
    "section": "Slides",
    "text": "Slides\nHere is a pdf export of the Keynote slides I did."
  },
  {
    "objectID": "posts/2011-03-15-psychology-as-a-reproducible-science.html",
    "href": "posts/2011-03-15-psychology-as-a-reproducible-science.html",
    "title": "Psychology as a reproducible Science",
    "section": "",
    "text": "Is Psychology ready for reproducible research?\nToday the typical research process in psychology looks generally like this: we collect data; analyze them in many ways; write a draft article based on some of the results; submit the draft to a journal; maybe produce a revision following the suggestions of the reviewers and editors; and hopefully live long enough to actually see it published. All of these steps are closed to the public except for the last one ‚Äì the publication of the (often substantially) revised version of the paper. Journal editors and reviewers evaluate the written work submitted to them, they trust that the analyses described in the submission are done in a principled and correct way. Editors and reviewers are the only external part of this process who will have an active influence on what analyses are done. After the publication of an article the public has the opportunity to write comments or ask the authors for the actual datasets for re-analysis. Often however, getting access to data from published papers is hard, if not often impossible (Savage & Vickers, 2009; Wicherts, Borsboom, Kats, & Molenaar, 2006). ¬†Unfortunately only the gist of the analyses are described in the paper and neither exact verification nor innovative additional analyses are possible.\nWhat could be a solution for this problem? An example from computer science provides a concept called ‚Äúliterate programming‚Äù which was advocated by one of the field‚Äôs grandmasters, Donald Knuth, in 1984. Knuth suggested that documentation (comments in the code) should be just as important as the actual code itself. This idea was reflected nearly 20 years later when Schwab et al.¬†(2000) formulated a concept that ‚Äúreplication by other scientists‚Äù is a central aim and guardian for intellectual quality; they coined the term ‚Äúreproducible research‚Äù for such a process.\nLet‚Äôs move the research process to a more open, reproducible structure, in which scientific peers have the ability to evaluate not only the final publication but also the data and the analyses.\nIdeally, research papers would have code for analyses which are commented in detail and are submitted in tandem with drafts as well as the original datasets. Anybody, not only a restricted group of select reviewers and editors, could reproduce all the steps of the analysis and follow the logic of arguments on not only the conceptual level but at an analytic level as well. This openness facilities easy reanalysis of data also. Meta-analysis could be done more frequently and with greater resolution as the actual data are available. Moreover, this configuration would allow us collectively to estimate effects in the population and not restrict our attention to independent small samples (see Henrich, Heine, & Norenzayan, 2010 for a discussion of this topic).\nWhat do we need to achieve this? From a policy perspective, journals would have to add the requirement for data and code submission together with the draft of each empirical paper. Some journals already provide the option to do that (e.g., Behavior Research Methods) in the supplemental material section on a voluntary base, some require the submission of all necessary material to replicate the reported results (e.g., Econometrica), however most do not offer such a possibility (it is of course possible to provide such materials through private or university web sites, but this is a haphazard and decentralized arrangement).\nTools are a second important part of facilitating this openness. Three open source (free of cost) components could provide the bases for reproducible research:\n\nR (R Development Core Team, 2010) is widely recognized (cite) as the ‚Äúlanguage of statistics‚Äù and builds on writing code instead of a ‚Äúclick and forget‚Äù type of analysis that other software packages encourage. R is open source, comes with a large number of extensions for advanced statistical analysis and can be run on any computer platform, including as a Web based application (http://www.R-project.org).\nLaTeX was invented to provide a tool for anybody to produce high quality publications independent of the computer system used (i.e.¬†one could expect the same results everywhere, http://www.latex-project.org/).\nSweave (Leisch, 2002) connects R and LaTeX providing the opportunity to write a research paper and do the data analysis in parallel, in a well documented and reproducible way (http://www.stat.uni-muenchen.de/~leisch/Sweave/).\n\nThe power of these different tools comes from the combination of their being open source, their widespread adoption (across a wide range of fields in sciences), and the fully transparent means by which data analysis is conducted and reported. ¬†It levels the playing field and means that anybody with an Internet connection and a computer can take part in evolving scientific progress.\nJohn Godfrey Saxe famously said that: ‚ÄúLaws, like sausages, cease to inspire respect in proportion as we know how they are made.‚Äù We should strive that this is not true for psychology as a science."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Things to come in Quarto",
    "section": "",
    "text": "So Quarto it is ‚Ä¶ after the 3rd time trying to update Wowchemy and Hugo and things breaking - time for something new ‚Ä¶ Quarto ‚ÄúQuarto¬Æ is an open-source scientific and technical publishing system built on Pandoc‚Äù ‚Ä¶\nAnd things have been really smooth ‚Ä¶ some basic setup of the quarto page, converting the .Rmd files I had to .qmd files (which have very similar syntax) and then! quarto publish netlify which simply pushes everything into a new Netlify repo - it can‚Äôt get much easier than that!"
  },
  {
    "objectID": "posts/2016-07-12-before-r-was-r-there-was-s.html",
    "href": "posts/2016-07-12-before-r-was-r-there-was-s.html",
    "title": "Before R there was S",
    "section": "",
    "text": "Before there was R, there was S. R was modeled on a language developed at AT&T Bell Labs starting in 1976¬†by Rick Becker and John Chambers (and, later, Alan Wilks) along with¬†Doug Dunn, Jean McRae, and Judy Schilling.\n\n\nHere is a¬†talk by Rick Becker telling the story of R. Good Stuff!"
  },
  {
    "objectID": "posts/2018-01-09-blind-haste-aka-im-blindflug.html",
    "href": "posts/2018-01-09-blind-haste-aka-im-blindflug.html",
    "title": "Blind Haste (aka im Blindflug)",
    "section": "",
    "text": "Chance encounters sometimes lead to interesting and new projects. This is one of those cases ‚Ä¶ I got to know Emanuel de Bellis¬†during my time at Nestle and we never stopped collaborating ever since (he actually lead my 2017 ‚Äòskype to‚Äô statistics with my wife as a close second ‚Ä¶)\nWe got our hands on a rich dataset of speed measures the police in Z√ºrich (Switzerland) does unbeknownst to the drivers during they year for planning purposes. The radar is put into a small black box hardly anybody realises driving by:\n\n(it‚Äôs the black box above the bin ‚Äì not the bin!)\nSo ‚Äì we got these data from over a million cars and got to work with them trying to find an answer to a question in perception research: Do people perceive their environment differently when light conditions deteriorate? and (even more important) Do drivers change their driving speed accordingly.\nWell ‚Äì they don‚Äôt ‚Ä¶ and here is correlational proof üôÇ\nStay tuned for a causal demonstration ‚Äì oh yes!\n\nBlind haste: As light decreases, speeding increases\nWorldwide, more than one million people die on the roads each year. A third of these fatal accidents are attributed to speeding, with properties of the individual driver and the environment regarded as key contributing factors. We examine real-world speeding behavior and its interaction with illuminance, an environmental property defined as the luminous flux incident on a surface. Drawing on an analysis of 1.2 million vehicle movements, we show that reduced illuminance levels are associated with increased speeding. This relationship persists when we control for factors known to influence speeding (e.g., fluctuations in traffic volume) and consider proxies of illuminance (e.g., sight distance). Our findings add to a long-standing debate about how the quality of visual conditions affects drivers‚Äô speed perception and driving speed. Policy makers can intervene by educating drivers about the inverse illuminance‚Äíspeeding relationship and by testing how improved vehicle headlights and smart road lighting can attenuate speeding."
  },
  {
    "objectID": "posts/2019-06-20-handbook-of-process-tracing-methods.html",
    "href": "posts/2019-06-20-handbook-of-process-tracing-methods.html",
    "title": "Handbook of Process Tracing Methods",
    "section": "",
    "text": "WOOP WOOP - here it is - the second edition of our beloved ‚ÄúHandbook of Process Tracing Methods‚Äù\nIf you can‚Äôt wait - buy it here\nIt is bigger and better than the first edition, comes with the classics (Figner on skin conductance, Willemsen on Mouselab and many more) and many new awesome chapters - here is a list:\n1 Eye Fixations as a Process Trace - J. Edward Russo\n2 Pervasive Eye-Tracking for Real-World Consumer Behavior Analysis - Andreas Bulling and Michel Wedel\n3 Investigating Pupil Dilation in Decision Research - Joseph Tao-yi Wang and Wei James Chen\n4 A Primer on Eye-Tracking Methodology for Behavioral Science - Jacob L. Orquin and Kenneth Holmqvist\n5 Increasing Reproducibility of Eye-Tracking Studies: The EyeGuidelines - Susann Fiedler, Michael Schulte-Mecklenbeck, Frank Renkewitz, and Jacob L. Orquin\n6 (Re)Visiting the Decision Factory: Observing Cognition with MouselabWEB - Martijn C. Willemsen and Eric J. Johnson\n7 Comparing Process Tracing Paradigms: Tracking Attention via Mouse and Eye Movements - Ana M. Franco-Watkins, Hayden K. Hickey, and Joseph G. Johnson\n8 Mouse-Tracking: A Practical Guide to Implementation and Analysis - Pascal J. Kieslich, Felix Henninger, Dirk U. Wulff, Jonas M. B. Haslbeck, and Michael Schulte-Mecklenbeck\n9 Mouse-Tracking: Detecting Types in Movement Trajectories - Dirk U. Wulff, Jonas M. B. Haslbeck, Pascal J. Kieslich, Felix Henninger, and Michael Schulte-Mecklenbeck\n10 Mouse-Tracking to Understand Real-Time Dynamics of Social Cognition - Benjamin S. Stillerman and Jonathan B. Freeman\n11 Measuring Electrodermal Activity and Its Applications in Judgment and Decision-Making Research - Bernd Figner, Ryan O. Murphy, and Paul Siegel\n12 Response Times as Identification Tools for Cognitive Processes Underlying Decisions - Mario Fifiƒá, Joseph W. Houpt, and J√∂rg Rieskamp\n13 A Practical Guide for Automated Facial Emotion Classification - Sabrina St√∂ckli, Michael Schulte-Mecklenbeck, Stefan Borer, and Andrea C. Samson\n14 EEG and ERPs as Neural Process Tracing Methodologies in Decision-Making Research - Mary E. Frame\n15 Decision Neuroscience: fMRI Insights into Choice Processes - Vinod Venkatraman and Crystal Reeck\n16 Probing the Decisional Brain with Noninvasive Brain Stimulation - Nad√®ge Bault, Elena Rusconi, and Giorgio Coricelli\n17 Verbal Reports and Decision Process Analysis - Rob Ranyard and Ola Svenson\n18 Thinking Aloud during Superior Performance on Tasks Involving Decision Making - K.Anders Ericsson and Jerad H. Moxley\n19 Tracking Free Information Access: The Method of Active Information Search - Oswald Huber, Anton K√ºhberger, and Michael Schulte-Mecklenbeck\n20 Uncovering the Anatomy of Search Without Technology - Dirk U. Wulff and Ralph Hertwig\n21 Process Tracing, Sampling, and Drift Rate Construction - Neil Stewart and Timothy L. Mullett\n22 Using Multiple Methods to Elicit Choices and to Identify Strategies - Ulrich Hoffrage and Nils Reisen\n23 Testing Cognitive Models by a Joint Analysis of Multiple Dependent Measures Including Process Data - Andreas Gl√∂ckner and Marc Jekel\n24 Using Process Tracing Data to Define and Test Process Models - Joseph G. Johnson and Mary E. Frame"
  },
  {
    "objectID": "posts/2014-09-25-dplyr-is-growing-up.html",
    "href": "posts/2014-09-25-dplyr-is-growing-up.html",
    "title": "dplyr is growing up ‚Ä¶",
    "section": "",
    "text": "dplyr is the new plyr ‚Äì and it is awesome!\nfast, consistent and easy to read ‚Ä¶ check out a set of instructional pages, presentation and videos here\nThanks Hadley Wickham"
  },
  {
    "objectID": "posts/2017-04-12-something-about-reverse-inference.html",
    "href": "posts/2017-04-12-something-about-reverse-inference.html",
    "title": "Something about reverse inference",
    "section": "",
    "text": "Often, when we run process tracing studies (e.g., eye-tracking, mouse-tracking, thinking-aloud) we talk about cognitive processes (things we can‚Äôt observe) in a way that they are actually and directly observable. This is pretty weird ‚Äì which becomes obvious when looking at the data from the paper below. In this paper we simply instruct participants to follow a strategy when making choices between risky gamble problems. Taking the example of fixation duration we see that there is surprisingly litte difference between calculating an expected value, using a heuristic (priority heuristic) and just making decisions without instructions (no instruction) ‚Ä¶ maybe we should rethink our mapping of observation to cognitive processes a bit?\nHere is the paper:\nSchulte-Mecklenbeck, M., K√ºhberger, A., Gagl, S., & Hutzler, F. (in press). Inducing thought processes: Bringing process measures and cognitive processes closer together. Journal of Behavioral Decision Making.\nAbstract: The challenge in inferring cognitive processes from observational data is to correctly align overt behavior with its covert cognitive process. To improve our understanding of the overt‚Äìcovert mapping in the domain of decision making, we collected eye-movement data during decisions between gamble-problems. Participants were either free to choose or instructed to use a specific choice strategy (maximizing expected value or a choice heuristic). We found large differences in looking patterns between free and instructed choices. Looking patterns provided no support for the common assumption that attention is equally distributed between outcomes and probabilities, even when participants were instructed to maximize expected value. Eye-movement data are to some extent ambiguous with respect to underlying cognitive processes."
  },
  {
    "objectID": "posts/2010-03-18-latex-looks-more-scientific.html",
    "href": "posts/2010-03-18-latex-looks-more-scientific.html",
    "title": "LaTeX looks more scientific",
    "section": "",
    "text": "There are long discussions on the benefits of LaTeX over Word, but this statement from a (not too serious) paper of Andrew Gelman (a Professor of Statistics and Political Sciences at Columbia University) hits the spot:"
  },
  {
    "objectID": "posts/2008-03-17-priority-heuristic-comment.html",
    "href": "posts/2008-03-17-priority-heuristic-comment.html",
    "title": "Priority Heuristic comment",
    "section": "",
    "text": "We Johnson, Schulte-Mecklenbeck, & Willemsen, 2008 have got a new paper out that comments on the Priority Heuristic as described in Brandstaetter, Gigerenzer and Hertwig, 2006.\nResolution of debates in cognition usually comes from the introduction of constraints in the form of ne3, in contrast, has relied predominantly on testing models by examining their fit to choices. The authors examine a recently proposed choice strategy, the priority heuristic, which provides a novel account of how people make risky choices. The authors identify a number of properties that the priority heuristic should have as a process model and illustrate how they may be tested. The results, along with prior research, suggest that although the priority heuristic captures some variability in the attention paid to outcomes, it fails to account for major characteristics of the data, particularly the frequent transitions between outcomes and their probabilities. The article concludes with a discussion of the properties that should be captured by process models of risky choice and the role of process data in theory development."
  },
  {
    "objectID": "posts/2012-09-21-r-style-guide.html",
    "href": "posts/2012-09-21-r-style-guide.html",
    "title": "R Style Guide",
    "section": "",
    "text": "This is mainly a note to self:\nThere are several style guides for R out there. I particularly like the one from Google¬†and the somewhat lighter version of Hadley (ggplot god).\nAll of that style guide thinking started after a question on &lt;stackoverflow.com&gt; about R workflow ‚Ä¶ How do we organize large R projects. Hadley (again) is favoring an Load-Clean-Func-Do approach which looks somewhat like that:\n\nload.R # load data\nclean.R # clean up crap\nfunc.R # add functions\ndo.R # do the work"
  },
  {
    "objectID": "posts/2009-12-12-how-weird-subjects-can-be-overcome-a-comment-on-henrich-et-al.html",
    "href": "posts/2009-12-12-how-weird-subjects-can-be-overcome-a-comment-on-henrich-et-al.html",
    "title": "How WEIRD subjects can be overcome ‚Ä¶ a comment on Henrich et al.",
    "section": "",
    "text": "Joe Henrich published a target article in BBS talking about how economics and psychology base their research on WEIRD (Western, Educated, Industrialized, Rich and Democratic) subjects.\nHere is the whole abstract:\n‚Äî\nBehavioral scientists routinely publish broad claims about human psychology and behavior in the world‚Äôs top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich and Democratic (WEIRD) societies. Researchers‚Äîoften implicitly‚Äîassume that either there is little variation across human populations, or that these ‚Äústandard subjects‚Äù are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species‚Äîfrequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior‚Äîhence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.\n‚Äî\nI would like to make three suggestions that could help to overcome the era of WEIRD subjects and generate more reliable and representative data. These suggestions will mainly touch contrasts 2, 3 and 4 elaborated by Henrich, Heine and Norezayan. While my suggestions tackle these contrasts from a technical and experimental perspective they do not provide a general solution for the first contrast on industrialized versus small scale societies. Here are my suggestions: 1) replications in multiple labs, 2) internet based experimentation and 3) drawing representative samples from a population.\nThe first suggestion, replication in multiple labs, foremost touches aspects like replication, multiple populations and open data access. For a publication in a journal a replication of an experiment in a different lab would be obligatory. The replication would then be published with the original, e.g., in the form of a comment. This would ensure that other research labs in other states or countries are involved and very different parts of the population could be sampled. Also results of experiments would be freely available to the public and the data sharing problem in Psychology, as described in the target article, but also in other fields like Medicine (Savage & Vieckers, 2009) would be a problem of the past. Of course such a step would be closely linked with certain standards on the one hand in building experiments and on the other hand in storing data. While a standard way to build experiments seems unlikely there are many methods available in computer science to store data in a reusable, for example through the usage of XML (Extensible Markup Language).\nThe second suggestion is based on the drawing of representative samples from the population. As described in the target article, research often suffers from a restriction to extreme subgroups from the population, from which generalized results are drawn. However, there is published work that overcomes these restrictions. As an example I would like to use the Hertwig, Zangerl, Biedert and Margraf (2008) paper on probabilistic numeracy. The authors based their study on a random-quote sample from the Swiss population including indicators as language, area where participant is living, gender and age. To fulfill all the necessary criteria 1000 participants were recruited using telephone interviews. Such studies are certainly more expensive and somewhat restricted to simpler experimental setups (Hertwig et al., used telephone interviews based on questionnaires).\nThe third suggestion adds additional data collection in a second location: the Internet. The emphasis in the last sentence should be set on ‚Äòadd‚Äô. Data collection solely Internet based is of course possible, already often performed and published in high impact journals. Online experimentation is technically much less demanding than ten years ago due to the availability of ready made solutions for questionnaires or even experiments. The point I would like to make here should not be built on a separation of lab and online based experiments. My suggestion combines these two research locations and enables a researcher to profit from the many benefits arising. A possible scenario could include running an experiment in the laboratory first to guarantee, among other things, high control on the situation in order to show an effect with a small, restricted sample. In a second step the experiment is transferred to the Web and run online, admittedly giving away some of the control but providing the large benefit of having access to a diverse, large samples of participants from different populations easily. As an example I would like to point to a recent blog and related experiments started by Paolacci and Warglien (2009) at the University of Venice, Italy. These researchers started replicating well known experiments from the decision making literature like framing, anchoring or the conjunction fallacy with a service called the Mechanical Turk provided by Amazon. This service is based on the idea of crowdsourcing (outsourcing a task to a large group of people) and lets a researcher have easy access to a large group of motivated participants.\nSome final words on the combination and possible restrictions of the three suggestions. What would a combination of all three suggestions look like? It would be a replication of experiments, using representative samples of different populations in online experiments. This seems useful from a data quality, logistics and prize point of view. However, several issues were left untouched in my discussion, such as the question of independence of the second lab for replication studies, the restriction of representative samples to one country (as opposed to multiple comparisons as routinely found in, e.g., anthropological studies), the differences between online and lab based experimentation or the instances where equipment needed for an experiments (e.g., eye trackers or fMRI) does not allow for online experimentation. Keeping that in mind the above suggestions draw an idealized picture of how to run experiments and re-use the collected data, nevertheless I would argue that such steps could help to reduce the percentage of WEIRD subjects in research substantially.\nReferences\nHertwig, R., Zangerl, M.A., Biedert, E., & Margraf, J. (2008). The Public‚Äôs Probabilistic Numeracy: How Tasks, Education and Exposure to Games of Chance Shape It. Journal of Behavioral Decision Making, 21, 457-570.\nPaolacci, G., & Warglien, M. (2009). Experimental turk: A blog on social science experiments on Amazon Mechanical Turk. Accessed on November 17th 2009:\nSavage, C.J., & Vickers, A.J. (2009). Empirical Study of Data Sharing by Authors Publishing in PLoS Journals. PLoS ONE 4(9): e7078.doi:10.1371/journal.pone.0007078"
  },
  {
    "objectID": "posts/2022-11-16-numeracy-dm/index.html",
    "href": "posts/2022-11-16-numeracy-dm/index.html",
    "title": "Replication: Numeracy and decision making",
    "section": "",
    "text": "A series of four studies explored how the ability to comprehend and transform probability numbers relates to performance on judgment and decision tasks. On the surface, the tasks in the four studies appear to be widely different; at a conceptual level, however, they all involve processing numbers and the potential to show an influence of affect. Findings were consistent with highly numerate individuals being more likely to retrieve and use appropriate numerical principles, thus making themselves less susceptible to framing effects, compared with less numerate individuals. In addition, the highly numerate tended to draw different (generally stronger or more precise) affective meaning from numbers and numerical comparisons, and their affective responses were more precise. Although generally helpful, this tendency may sometimes lead to worse decisions. The less numerate were influenced more by competing, irrelevant affective considerations. Analyses showed that the effect of numeracy was not due to general intelligence. Numerical ability appears to matter to judgments and decisions in important ways."
  },
  {
    "objectID": "posts/2022-11-16-numeracy-dm/index.html#abstract",
    "href": "posts/2022-11-16-numeracy-dm/index.html#abstract",
    "title": "Replication: Numeracy and decision making",
    "section": "",
    "text": "A series of four studies explored how the ability to comprehend and transform probability numbers relates to performance on judgment and decision tasks. On the surface, the tasks in the four studies appear to be widely different; at a conceptual level, however, they all involve processing numbers and the potential to show an influence of affect. Findings were consistent with highly numerate individuals being more likely to retrieve and use appropriate numerical principles, thus making themselves less susceptible to framing effects, compared with less numerate individuals. In addition, the highly numerate tended to draw different (generally stronger or more precise) affective meaning from numbers and numerical comparisons, and their affective responses were more precise. Although generally helpful, this tendency may sometimes lead to worse decisions. The less numerate were influenced more by competing, irrelevant affective considerations. Analyses showed that the effect of numeracy was not due to general intelligence. Numerical ability appears to matter to judgments and decisions in important ways."
  },
  {
    "objectID": "posts/2022-11-16-numeracy-dm/index.html#framing",
    "href": "posts/2022-11-16-numeracy-dm/index.html#framing",
    "title": "Replication: Numeracy and decision making",
    "section": "Framing",
    "text": "Framing\nSo, lets get started with some framing tasks ‚Ä¶ we use the same paradigm as Peters et al.¬†(2006) in that we let a participants judge a Psych student‚Äôs test score either in a positive or negative frame. An example for a positive frame was Emily got 74% of her test correct the connected negative version was: Emily got 26% of her test incorrect. We then varied the percentage correct [81, 78, 74, 71, 68] and incorrect and the names of students [Emily, Fabian, Anna, Luca, Laura] in a between subjects design, where participants were either subjected to the positive or negative framing condition.\nAveraging these ratings we find a somewhat small but, as we will see, significant difference between positive and negative framing.\n\n\n\n\n\nFig. 2: Framing separated by task.\n\n\n\n\nLets do a simple ANOVA and compare these means for Numeracy [high|low] x Framing [pos|neg].\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nframe         1     81    81.2    76.8 &lt;2e-16 ***\nResiduals   928    981     1.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlright, framing effect ‚úì with a mean rating of M = 0.82 (SD = 1.21) for ‚Äònegative‚Äô and M = 1.41 (SD = 0.82) for ‚Äòpositive‚Äô - not a total disaster ‚Ä¶ which can be confirmed by calculation of Cohen‚Äôs d = 0.57.\n\nSome checking\nBefore we get going into the details, I will do some checking whether the tasks worked. In what follows I split the data for positive and negative frame from Fig. 2 into the 5 task variations we used. Remember that these have decreasing distance between the positive and negative framing condition - the positive percentages correct are: [81, 78, 74, 71, 68]. The first number in the column headings represent the negative frame, the second the positive, hence the first column is the 19% negative v. 81% positive condition and so on. Participants react to these differences with decreasing general ratings when the negative result increases. Pretty good!\n\n\n\n\n\nFig. 3: Sanity checks.\n\n\n\n\nThe Framing effect is small in the 19-81 condition and increases with increasing negative and connected decreasing positive percentage. I am using the map() function to run an Anova for each task_id - which will give us 5 Anovas with 2 lines each in the results tibble - here is the code that does that - a simple t-test would do the same, but could not figure out how to map that ‚Ä¶\n\nframing %&gt;% \n  nest(data = -task_id) %&gt;% \n  mutate(model = map(data, ~ anova(lm(rating ~ frame, .)))) %&gt;% \nselect(task_id, model) %&gt;% \nunnest(model)\n\n# A tibble: 10 √ó 6\n   task_id    Df `Sum Sq` `Mean Sq` `F value`  `Pr(&gt;F)`\n     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1       1     1    15.5     15.5       17.7   4.02e- 5\n 2       1   184   161.       0.877     NA    NA       \n 3       2     1     6.42     6.42       8.38  4.26e- 3\n 4       2   184   141.       0.767     NA    NA       \n 5       3     1    41.9     41.9       48.5   5.72e-11\n 6       3   184   159.       0.865     NA    NA       \n 7       4     1     7.77     7.77       9.62  2.23e- 3\n 8       4   184   149.       0.808     NA    NA       \n 9       5     1    19.4     19.4       21.5   6.66e- 6\n10       5   184   166.       0.904     NA    NA       \n\n\nPretty neat - and confirming what we suspected above - significant framing effect for each task and difference between results for the rated student."
  },
  {
    "objectID": "posts/2022-11-16-numeracy-dm/index.html#the-interaction",
    "href": "posts/2022-11-16-numeracy-dm/index.html#the-interaction",
    "title": "Replication: Numeracy and decision making",
    "section": "The Interaction",
    "text": "The Interaction\nOf course, what we ultimately after is the interaction between framing and numeracy - remember, people high in numeric ability are less susceptible to framing - in theory ‚Ä¶\nLets first plot the interaction graph between framing x numeracy:\n\n\n\n\n\nFig. 4: Framing vs.¬†Numeracy.\n\n\n\n\nHmmmm - we clearly see a difference between positive and negative frame, knew that already, but the difference between the numeracy groups seems small ‚Ä¶\nI will run something a bit more complex - a multi level model where I use frame and numeracy (score) as predictors. I also add a random intercept for participants (ID) and nest task_id into that.\n\nframing_model &lt;- lmer(rating ~ frame * score + (1|task_id) + (1|ID), data = framing)\nsummary(framing_model)  \n\nLinear mixed model fit by REML ['lmerMod']\nFormula: rating ~ frame * score + (1 | task_id) + (1 | ID)\n   Data: framing\n\nREML criterion at convergence: 1928\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-7.729 -0.495  0.001  0.519  4.451 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.571    0.756   \n task_id  (Intercept) 0.260    0.510   \n Residual             0.277    0.527   \nNumber of obs: 930, groups:  ID, 186; task_id, 5\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)     0.664587   0.270993    2.45\nframepos        0.602860   0.200104    3.01\nscore           0.082348   0.062650    1.31\nframepos:score -0.000819   0.086923   -0.01\n\nCorrelation of Fixed Effects:\n            (Intr) framps score \nframepos    -0.397              \nscore       -0.447  0.606       \nframeps:scr  0.322 -0.814 -0.721\n\n\nImmediately we run into the problem that the output, in principle, has all the necessary information but it would be great to have this as a bit nicer overview - here the sjPlot package is super useful - it has a function called tab_model() that takes the output of lmer() as an input and provides well formatted html-ouput:\n\ntab_model(framing_model,\n          show.df = TRUE, \n          show.stat = TRUE)\n\n\n\n\n¬†\nrating\n\n\nPredictors\nEstimates\nCI\nStatistic\np\ndf\n\n\n(Intercept)\n0.66\n0.13¬†‚Äì¬†1.20\n2.45\n0.014\n923.00\n\n\nframe [pos]\n0.60\n0.21¬†‚Äì¬†1.00\n3.01\n0.003\n923.00\n\n\nscore\n0.08\n-0.04¬†‚Äì¬†0.21\n1.31\n0.189\n923.00\n\n\nframe [pos] √ó score\n-0.00\n-0.17¬†‚Äì¬†0.17\n-0.01\n0.992\n923.00\n\n\nRandom Effects\n\n\n\nœÉ2\n0.28\n\n\n\nœÑ00 ID\n0.57\n\n\nœÑ00 task_id\n0.26\n\n\nICC\n0.75\n\n\nN task_id\n5\n\n\nN ID\n186\n\nObservations\n930\n\n\nMarginal R2 / Conditional R2\n0.082 / 0.770\n\n\n\n\n\n\nWhile, as expected, framing results in a significant effect neither score (which referes to numeracy in its continuous form) nor the interaction between framing x score results in a significant effect, hence somewhat failed?!"
  },
  {
    "objectID": "posts/2014-05-07-when-something-old.html",
    "href": "posts/2014-05-07-when-something-old.html",
    "title": "When something old ‚Ä¶",
    "section": "",
    "text": "Schulte-Mecklenbeck, M., & K√ºhberger, A. (2014).¬†Out of sight ‚Äì out of mind? Information acquisition patterns in risky choice framing. Polish Psychological Bulletin, 45, 21‚Äì28.\n\n\nI teamed up with Anton K√ºhberger to write about one of our old, favorite topics: framing and process tracing ‚Ä¶\n\n\nHere is the abstract: We investigate whether risky choice framing, i.e., the preference of a sure over an equivalent risky option when choosing among gains, and the reverse when choosing among losses, depends on redundancy and density of information available in a task. Redundancy, the saliency of missing information, and density, the description of options in one or multiple chunks, was manipulated in a matrix setup presented in MouselabWeb. On the choice level we found a framing effect only in setups with non-redundant information. On the process level outcomes attracted more acquisitions than probabilities, irrespective of redundancy. A dissociation between acquisition behavior and choice calls for a critical discussion of the limits of process-tracing measures for understanding and predicting choices in decision making tasks."
  },
  {
    "objectID": "posts/2019-06-04-HoPTM2ndEdition.html",
    "href": "posts/2019-06-04-HoPTM2ndEdition.html",
    "title": "BernR Meetup",
    "section": "",
    "text": "Today (Dec 10th 2018) we will meet for the first BernR Meetup (https://www.meetup.com/Bern-R/) ‚Äì hope to learn new things and get to know cool R people. More to follow soon .."
  },
  {
    "objectID": "posts/2015-02-20-schnitzeljagd.html",
    "href": "posts/2015-02-20-schnitzeljagd.html",
    "title": "Schnitz(e)ljagd",
    "section": "",
    "text": "Find the Schnitz(e)l!"
  },
  {
    "objectID": "posts/2013-08-16-new-paper-on-food-choice-and-simple-heuristics.html",
    "href": "posts/2013-08-16-new-paper-on-food-choice-and-simple-heuristics.html",
    "title": "New paper on food choice and simple heuristics",
    "section": "",
    "text": "We got a new paper out on how people (consumers) use simple rules to make food choices. This is work in collaboration with the Nestl√© Research Center in Lausanne.\nHere is the reference:\n\nSchulte-Mecklenbeck, M., Sohn, M., Bellis, E., Martin, N., & Hertwig, R. (2013). A Lack of Appetite for Information and Computation: Simple Heuristics in Food Choice. Appetite, 71, 242‚Äì251.\n\nAbstract\nThe predominant, but largely untested, assumption in research on food choice is that people¬†obey the classic commandments of rational behavior: they carefully look up every piece of¬†relevant information, weight each piece according to subjective importance, and then¬†combine them into a judgment or choice. In real world situations, however, the available¬†time, motivation, and computational resources may simply not suffice to keep these¬†commandments. Indeed, there is a large body of research suggesting that human choice is¬†often better accommodated by heuristics‚Äîsimple rules that enable decision making on the¬†basis of a few, but important, pieces of information. We investigated the prevalence of such¬†heuristics in a computerized experiment that engaged participants in a series of choices¬†between two lunch dishes. Employing MouselabWeb, a process-tracing technique, we found¬†that simple heuristics described an overwhelmingly large proportion of choices, whereas¬†strategies traditionally deemed rational were barely apparent in our data. Replicating previous¬†findings, we also observed that visual stimulus segments received a much larger proportion¬†of attention than any nutritional values did. Our results suggest that, consistent with human¬†behavior in other domains, people make their food choices on the basis of simple and¬†informationally frugal heuristics."
  },
  {
    "objectID": "posts/2017-10-31-a-short-history-of-process-tracing.html",
    "href": "posts/2017-10-31-a-short-history-of-process-tracing.html",
    "title": "A short history of process tracing",
    "section": "",
    "text": "Finally out (already mentioned earlier this year) ‚Äì now in it‚Äôs full glory @ Current Directions in Psychological Science.\nGood Stuff.\n\n\na little process tracing history together with the delightful @dggoldst https://t.co/svGBLYcAZv#processtracing #JDM\n\n\n‚Äî Michael Schulte (@SchulteMi) October 31, 2017"
  },
  {
    "objectID": "posts/2019-06-25-what-hoptm-looks-like-from-the-inside.html",
    "href": "posts/2019-06-25-what-hoptm-looks-like-from-the-inside.html",
    "title": "What HoPTM looks like from the inside",
    "section": "",
    "text": "As mentioned some days ago our Handbook of Process Tracing Methods is out in the wild ‚Ä¶\nHere is a bit of an overview of what is going on inside :)\nThe book has 390 pages divided into 24 chapters. There are 202014 words in there including everything (references, thanks, hello, goodbye ‚Ä¶).\nIgnoring the chapters and that they have reference lists, that mess up things a bit, first an overview of frequency for highly frequent words:\n\nNo big surprises there for a process tracing book in decision making - but still - models pop up quite high in the list and trajectories - the new kid on the block when it comes to mousetracking also gets a mention - the list is truncated and I did not bother to remove things like ‚Äòet al.‚Äô or ‚Äòe.g‚Äô (the second . got caught by the script, the first one not).\n\nThis is more interesting - I calculated the tf-idf (term frequency‚Äìinverse document frequency) which tells us how important a word is but takes care of how often a word appears in general - so a better measure than just using the raw frequency of a word. I did this for each chapter (chapter names are listed in the other post). At first I wanted to remove the reference list but I realized that it provides an interesting insight - namely how often authors cite themselves in their chapters. A caveat in this graph is that the x-axis has different scaling. Apart from that it actually describes the content of some of the chapters pretty well - ah and I just saw that I should have added some stemming to avoid ‚Äòclusters‚Äô and ‚Äòcluster‚Äô being listed separatly ‚Ä¶ but well ‚Ä¶\nFor the kicks - here is a network diagram of bigrams. There is clearly some more work to do here ‚Ä¶\n\nShoutout to the tidytext people - you got a great package there!"
  },
  {
    "objectID": "posts/2009-12-22-accepting-to-fail-in-the-name-of-science.html",
    "href": "posts/2009-12-22-accepting-to-fail-in-the-name-of-science.html",
    "title": "Accepting to fail (in the name of science)",
    "section": "",
    "text": "Very interesting article in WIRED on accepting failure and how ignoring it changes the way scientists make progress (or not).\nGood theme for new years resolutions ‚Ä¶\nIn the meantime: Happy Holidays!"
  },
  {
    "objectID": "posts/2009-05-17-create-colored-title-in-r-plot.html",
    "href": "posts/2009-05-17-create-colored-title-in-r-plot.html",
    "title": "create colored title in R plot",
    "section": "",
    "text": "David Smith has a very nice code example in which he sets the color of title word in a plot to the actual grouping color. Code can be found here. This seems extremely useful for posters and presentations. I doubt however that journals would pick up on that ‚Ä¶"
  },
  {
    "objectID": "posts/2010-07-15-two-good-things-come-together.html",
    "href": "posts/2010-07-15-two-good-things-come-together.html",
    "title": "two good things come together",
    "section": "",
    "text": "https://docs.latexlab.org/docs\nLaTeX and Google Docs together in one nice (free)application ‚Äì this is a brilliant idea, which gives you the power of LaTeX combined with the excellent collaboration possiblities of Google Docs ‚Äì if I would have a button it would say ‚ÄúI like‚Äù ‚Ä¶"
  },
  {
    "objectID": "posts/2012-02-03-on-writing-in-social-science.html",
    "href": "posts/2012-02-03-on-writing-in-social-science.html",
    "title": "On writing in social science",
    "section": "",
    "text": "Malcom Gladwell (author of Blink and Tipping point) meets Dan Ariely (author of Predictively Irrational and Ig Nobel Prize Winner) to chat on what are good strategies in selecting topics to write about and the difficulties of wading through complex information ‚Ä¶ Its called a research chat"
  },
  {
    "objectID": "posts/2015-02-07-moving-an-idea-into-business.html",
    "href": "posts/2015-02-07-moving-an-idea-into-business.html",
    "title": "Moving an idea into business",
    "section": "",
    "text": "Recently Ryan Murphy and myself realised that a startup here in Berlin features ideas of our 2011¬†Flashlight paper.\nWell, the guys at attensee.com¬†did a great¬†job taking the idea we had much further we ever thought one would be able to take it¬†‚Ä¶\nHere is a feature I totally love ‚Äì a live heat map of what you are looking at ‚Ä¶ awesome!"
  },
  {
    "objectID": "posts/2016-08-12-everything-is-fucked.html",
    "href": "posts/2016-08-12-everything-is-fucked.html",
    "title": "Everything is fucked ‚Ä¶",
    "section": "",
    "text": "This syllabus of an¬†(obviously) awesome class has a ton of good reads:\nEverything is fucked: The¬†syllabus\nby¬†Sanjay Srivastava\nI would have two additions:\n\nA multi lab replication project on ego-depletion¬†(Hagger & Chatzisarantis, 2016)\nAnd the response from Roy Baumeister and Kathleen D. Vohs\n\nIt‚Äôs a really good statement of how f‚Ä¶ up things are (in addition to all the other good examples above) ‚Ä¶\n\n‚ÄúA new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.‚Äù ‚Äì Max Planck"
  },
  {
    "objectID": "posts/2010-08-18-where-you-end-up-having-a-phd.html",
    "href": "posts/2010-08-18-where-you-end-up-having-a-phd.html",
    "title": "Where you end up, having a PhD",
    "section": "",
    "text": "The illustrated guide from Kindergarten to PhD ‚Ä¶\nhttp://matt.might.net/articles/phd-school-in-pictures/"
  },
  {
    "objectID": "posts/2019-06-20-pictures.html",
    "href": "posts/2019-06-20-pictures.html",
    "title": "Pictures",
    "section": "",
    "text": "So, here we go - new blogdown site ‚Ä¶ thanks to Dan (https://twitter.com/dsquintana) to kicked me over the edge actually doing this ‚Ä¶\nThings are fine, the site is up - pictures are still linked back to my old wordpress site ‚Ä¶ will figure this out eventually ‚Ä¶ but this is live now - for your reading pleasure :)"
  },
  {
    "objectID": "posts/2015-02-12-all-that-mutate-and-summarise-beauty.html",
    "href": "posts/2015-02-12-all-that-mutate-and-summarise-beauty.html",
    "title": "all that mutate() and summarise() beauty",
    "section": "",
    "text": "The friendly people from RStudio recently started a webinar series¬†with talks on the following topics (among others):\nData¬†wrangling with R and RStudio\nThe Grammar and Graphics of Data Science (both dplyr happiness)\nRStudio and Shiny\n‚Ä¶ and many more.\nOur friend Dr.¬†Nathaniel D. Philipps also started a cool R course¬†with videos, shiny apps and¬†many other new goodies."
  },
  {
    "objectID": "posts/2013-06-03-limesurvey-randomizing.html",
    "href": "posts/2013-06-03-limesurvey-randomizing.html",
    "title": "Limesurvey randomizing",
    "section": "",
    "text": "It is kind of an odd problem.\nFor the following pretty straight forward question: How do I randomise questions within a group in Limesurvey? It seems to be really hard to find an answer.\nWith the help of Jonas¬†I figured out that there is a randomisation option hidden in the ‚ÄòAdvanced Settings‚Äô section of a question. What you have to do is provide the same number (this is important) for each question in a group that you want to randomise. Limesurvey will then take care of the rest. It did not seem to work if I named the variable with a string, eg, ‚Äògroup1‚Äô but only numeric counters work fine.\nThanks Jonas! (I would not have finished at all) ‚Ä¶"
  },
  {
    "objectID": "posts/2009-06-25-latex-tips.html",
    "href": "posts/2009-06-25-latex-tips.html",
    "title": "LaTeX tips",
    "section": "",
    "text": "Two things are often bothering when one starts to work with LaTeX: in text referencing of literature and Umlaute (for our German speaking friends) ****\nReferencing: Here is a list of 5 types of in text referencing. To get them to work in a LaTeX file you need to define a referencing style in your document (in the example I use APA ‚Äì American Psychological Association), you can of course use any style you want!\nIn the header:\nusepackage{apacite}\nbibliographystyle{apacite}\nIn the actual text part the command on the left results in the reference on the right:\ncite{Bortz1993} ‚Äì (Bortz, 1993)\nciteA{Bortz1993} ‚Äì Bortz (1993)\nciteNP{Campbell1959} ‚Äì Campbell, 1959\nciteauthor{Spiel2001} ‚Äì Spiel citeyearNP{Bortz1993} ‚Äì 1993\nAt the end of the document enter a reference to your .bib file, in the example below my file is called biblio.bib: bibliography{biblio}\nUmlaute: In German there are several Umlaute that are not included in the standard english setup of a LaTeX installation. There are two ways to get them right: 1) given a standard LaTeX document use (again command on the left, result on the right): ‚Äúa ‚Äì √§ ‚Äúu ‚Äì √º ‚Äúo ‚Äì √∂) given that you actually want to write in German do the following:\nusepackage[german]{babel}\nusepackage[ansinew]{inputenc}\nTwo useful links:\n1) Wikipedia book on LaTeX http://en.wikibooks.org/wiki/LaTeX\n2) A page at the University of Salzburg‚Äôs Psychology department (German!)\nGernot Kleiter was the person who introduced me to LaTeX, decision making, good experimenting ‚Ä¶ actually, there should be a whole post about him ‚Ä¶"
  },
  {
    "objectID": "posts/2009-09-06-r-flashmob.html",
    "href": "posts/2009-09-06-r-flashmob.html",
    "title": "R flashmob",
    "section": "",
    "text": "From: The R Flashmob Project\nSubject: R Flashmob #2\nYou are invited to take part in R Flashmob, the project that makes the\nworld a better place by posting helpful questions and answers about the\nR statistical language to the programmer‚Äôs Q & A site stackoverflow.com\nPlease forward this to other people you know who might like to join.\nFAQ\nQ. Why would I want to join an inexplicable R mob?\nA. Tons of other people are doing it.\nQ. Why else?\nA. Stackoverflow was built specifically for handling programming questions.\nIt‚Äôs a better mousetrap. It offers search (and is well indexed by search engines),\ntagging, voting, the ability to choose the ‚Äúbest‚Äù answer to a question, and the ability to\nedit questions and answers as technology progresses. It has a karma system to\nreward people who are happy to help and discourage MLJs (mailing list jerks).\nQ. Do the organizers of this MOB have any commercial interest in stackoverflow?\nA. None at all. We‚Äôre just convinced it is the best way to help and promote R. All\nthe content submitted to stackoverflow is protected by a Creative Commons\nCC-Wiki License, meaning anyone is free to copy, distribute, transmit, and\nremix the information on stackoverflow. All the content on stackoverflow is\nregularly made available for download by the public.\nINSTRUCTIONS ‚Äì R MOB #2\nLocation: stackoverflow.com\nStart Date: Tuesday, September 8th, 2009\nStart Time:\n10:04 AM ‚Äì US Pacific\n11:04 AM ‚Äì US Mountain\n12:04 PM ‚Äì US Central\n1:04 PM ‚Äì US Eastern\n6:04 PM ‚Äì UK\n7:04 PM ‚Äì Continental W. Europe\n5:04 AM (Weds) ‚Äì New Zealand (birthplace of R)\nDuration: 50 minutes\n\nAt some point during the day on September 8th, synchronize your watch to\nhttp://timeanddate.com/worldclock/personal.html?cities=137,75,64,179,136,37,22\nThe mob should form at precisely 4 minutes past the hour and not beforehand.\nAt 4 minutes past the hour, you should arrive at stackoverflow.com, log in,\nand post 3 R questions. Be sure to tag the questions ‚ÄúR‚Äù. See the posting\nguidelines at http://stackoverflow.com/faq to understand what makes a good\nquestion.\nFollow R Flashmob updates at http://twitter.com/rstatsmob\nPost twitter messages tagged #rstats and #rstatsmob during the mob,\nproviding links to your questions.\nDuring the R MOB, you can chat with other participants on the #R channel\non IRC (freenode). To do this, install the Chatzilla extension on Firefox.\nClick ‚Äúfreenode‚Äù on the main screen. Then type /join #R in the field at the\nbottom of the screen. Then chat.\nIf you finish posting your three questions within the 50 minutes, stick\naround to answer questions and give ‚Äúup votes‚Äù to good questions and answers.\nIMPORTANT: After posting, sign the R Flashmob guestbook at\nhttp://bit.ly/6F8B2\nReturn to what you would otherwise have been doing. Await\ninstructions for R MOB #3."
  },
  {
    "objectID": "posts/2018-07-30-a-letter-to-the-black-goat.html",
    "href": "posts/2018-07-30-a-letter-to-the-black-goat.html",
    "title": "A letter to the black goat",
    "section": "",
    "text": "I wrote this letter to the black goat podcast ‚Ä¶ will update here if I hear back from them ‚Ä¶\n¬†\nDear goaters (is this a good way to address the three of you?),\n\n\n\n\nI attended SIPS some weeks ago (first timer). I was unsure what to expect but got a lot of bang for my buck (which is great) ‚Äì as a side note I would recommend first timers to try to go there with a concrete project or question or problem, I think there is a good chance to find people with similar issues interested in collaborations.\n\n\n\n\n\nHere is an observation I would be interested to hear your thoughts on: in the SIPS world everything seems to be really straight forward ‚Äì we want to ‚Äòunfuck psych science‚Äô (Lindsay, 2018), we want to pre-reg studies, upload data, learn R, comment on code, accelerate science, ‚Äòdo it right this time‚Äô, collaborate, respect, be inclusive (I really learned a lot in that regard listening to your podcast and talking to people at SIPS) ‚Ä¶\n\n\nAll of that is great. I subscribe to all of these points.\n\n\n\n\n\nHere is the twist ‚Äì people @SIPS talk about ‚Äòa movement‚Äô ¬†(something that seems very American to me), maybe a movement is needed; people @SIPS talk about ‚Äòa revolution‚Äô ‚Äì again, great! obviously there is a need to rattle the cage and accelerate things beyond ‚Äòparadigm shifts at funerals‚Äô (Planck?)\n\n\nBUT\n\n\nWhat happens if you go ‚ÄòOutside the SIPS Bubble‚Äô (OSIPSB)?\n\n\nI have no data (other than my own experiences) but I wonder whether the world (psychology, other sciences) is actually that ready, that willing and open to adopt to these new standards and to actually make a paradigm shift in how we do science. I work at a business school (consumer psych and JDM) there is a lot of finger pointing going on toward psychology (I have a similar feeling that within psychology there is a lot of finger pointing toward social psych) ‚Äì ‚Äòthis is clearly a problem of psychologists but not of us [economists, consumer psychologists, business, accounting researchers ‚Ä¶]‚Äô.\n\n\nAnother OSIPSB experience I had was talking to an Action Editor of the Journal of Consumer Psychology (JCR) last year ‚Äì we got into a heated debate about the most basic issues, eg, sharing data, pre-reg studies ‚Ä¶\n\n\n\n\n\nIs this an observation you share? What would be good steps to address these issues? Should we talk about this within SIPS to get a better balance between enthusiasm and real world requirements (eg hiring decisions are still made mostly by senior faculty, assuming my observation hold, less interested in replication than ‚Äônew and exciting effects‚Äô (quote anonymous senior AE of JCR).\n\n\n\n\n\nThanks for your thoughts!\n\n\n\n\n\n\n\nUPDATE in 2019 - on my way to SIPS2019, casually started listening to some podcasts - and then this: https://www.podbean.com/ew/pb-2iqpx-b6aa1e"
  },
  {
    "objectID": "posts/2009-06-14-double-triple-or-even-sextuple-blind.html",
    "href": "posts/2009-06-14-double-triple-or-even-sextuple-blind.html",
    "title": "double, triple or even sextuple blind?",
    "section": "",
    "text": "There is an interesting discussion on how the scientific review process should be handled going on at orgtheory.net blog. The point is that the obvious shortcomings in the current review system (the authors know who the editor is (and vice versa), the reviewer knows (or can easily infer) who the author is ‚Ä¶) can be handle through triple blind reviews: authors, reviewers AND editors are included (anonymous upload to a webpage (id through a code), quatruple blind reviews: no one know who the editor of the journal is, quintimple blind: after publication of a paper the authors name is kept secret for some years or, and that‚Äôs the actual kicker: sextuple blind: there is no journal name any more ‚Äì just the paper and the users decide whether it is worth citing or not ‚Ä¶\nIn a more detailed way you can find this approach explained here"
  },
  {
    "objectID": "posts/2014-08-22-die-zeit-wissen-schreibt-uber-essensentscheidungen.html",
    "href": "posts/2014-08-22-die-zeit-wissen-schreibt-uber-essensentscheidungen.html",
    "title": "Die Zeit Wissen ‚Ä¶ schreibt √ºber Essensentscheidungen",
    "section": "",
    "text": "Mal was L√§ngeres zu unseren Lieblingsthemen: Essen und Entscheidungsforschung ‚Ä¶ Enjoy!\n2014-08-19_DieZeitWissen-wie-sie-beim-essen-die-richtigen-entscheidungen-treffen"
  },
  {
    "objectID": "posts/2017-11-13-the-root-of-the-problem.html",
    "href": "posts/2017-11-13-the-root-of-the-problem.html",
    "title": "The root of the problem",
    "section": "",
    "text": "One of the root causes of where we are (as a science) in psychology and many other disciplines in terms of reproducibility of key (and other) results could not be better summed up than by the man himself Daryl Bem (2002):\n‚ÄúIf a datum suggests a new hypothesis, try to find additional evidence for it elsewhere in the data. If you see dim traces of interesting patterns, try to reorganize the data to bring them into bolder relief. If there are participants you don‚Äôt like, or trials, observers, or interviewers who gave you anomalous results, drop them (temporarily). Go on a fishing expedition for something ‚Äî anything ‚Äî interesting.‚Äù\n‚ÄòGo on a fishing expidition‚Äô ‚Äì why should there come anything good from such advise? Bem goes on ‚Ä¶\n‚ÄúNo, this is not immoral (SIC!). The rules of scientific and statistical inference that we overlearn in graduate school apply to the ‚ÄúContext of Justification.‚Äù They tell us what we can conclude in the articles we write for public consumption, and they give our readers criteria for deciding whether or not to believe us. But in the ‚ÄúContext of Discovery,‚Äù there are no formal rules, only heuristics or strategies.‚Äù\nI disagree with this statement, because the idea of finding something through torturing the data (until they confess) is a hug source of false positive results. We find an effect and falsely conclude that something is there when in fact there is nothing. I found the above quote when reading this paper¬†by Zwaan, Etz, Lucas & Donnellan (2017) ‚Äì a target article for BBS¬†which presents six common arguments against replication and a set of really good responses for such discussions.\nHere are the six ‚Äòconcerns‚Äô the authors discuss:\n\n\n&lt;div class=\"layoutArea\"&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;p&gt;\n      Concern I: Context Is Too Variable&lt;br /&gt; Concern II: The Theoretical Value of Direct Replications is Limited&lt;br /&gt; Concern III: Direct Replications Are Not Feasible in Certain Domains&lt;br /&gt; Concern IV: Replications are a Distraction&lt;br /&gt; Concern V: Replications Affect Reputations&lt;br /&gt; Concern VI: There is no Standard Method to Evaluate Replication Results\n    &lt;/p&gt;\n    \n    &lt;p&gt;\n      Both are really good reads &#8211; for very different reasons.\n    &lt;/p&gt;\n    \n    &lt;p&gt;\n      &nbsp;\n    &lt;/p&gt;\n    \n    &lt;p&gt;\n      &#8212;\n    &lt;/p&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n\nBern, D. (2002). Writing the empirical journal article.¬†In Darley, J. M., Zanna, M. P., & Roediger III, H. L. (Eds) (2002). The Compleat Academic: A Career Guide. Washington, DC: American Psychological Association.\nZwaan, R. A., Etz, A., Lucas, R. E., & Donnellan, B. (2017, November 1). Making Replication Mainstream. Retrieved from psyarxiv.com/4tg9c"
  },
  {
    "objectID": "posts/2017-04-30-latex.html",
    "href": "posts/2017-04-30-latex.html",
    "title": "LaTeX <- pdfTeX, LuaTeX, XeTeX",
    "section": "",
    "text": "I have been using LaTeX (with pdfTeX) for 20+ years now ‚Äì to be honest I never bothered to understand the differences between the different TeX engines ‚Äì here is an excellent article by Graham Douglas explaining the ins and outs of the TeX engine(s).\n‚Ä¶ the article also features Overleaf¬†a shared TeX environment ‚Ä¶ sweet!"
  },
  {
    "objectID": "posts/2017-11-13-professor-priming-or-not.html",
    "href": "posts/2017-11-13-professor-priming-or-not.html",
    "title": "Professor priming - or not",
    "section": "",
    "text": "&lt;div class=\"layoutArea\"&gt;\n  &lt;div class=\"column\"&gt;\n    &lt;p&gt;\n      This was my first contribution to a¬†&lt;a href=\"https://www.psychologicalscience.org/publications/replication-dijksterhuis-van-knippenberg\"&gt;Registered Replication Report&lt;/a&gt; (RRR). Being one of 40 participating labs was an interesting exercise &#8211; it might seem straightforward to run the same study in different labs, but we learned that such small things as √º, √§ and √∂ can generate a huge amount of problems and work (read &lt;a href=\"https://en.wikipedia.org/wiki/UTF-8\"&gt;this&lt;/a&gt; if you are into these kind of things).\n    &lt;/p&gt;\n    \n    &lt;p&gt;\n      Here is one of the central results:\n    &lt;/p&gt;\n    \n    &lt;p&gt;\n      &lt;img class=\"aligncenter size-full wp-image-579\" src=\"/uploads//2017/11/Screen-Shot-2017-11-13-at-22.19.39.png\" alt=\"\" width=\"718\" height=\"609\" srcset=\"2017/11/Screen-Shot-2017-11-13-at-22.19.39.png 718w, 2017/11/Screen-Shot-2017-11-13-at-22.19.39-300x254.png 300w, 2017/11/Screen-Shot-2017-11-13-at-22.19.39-589x500.png 589w, 2017/11/Screen-Shot-2017-11-13-at-22.19.39-500x424.png 500w\" sizes=\"(max-width: 718px) 100vw, 718px\" /&gt;\n    &lt;/p&gt;\n    \n    &lt;p&gt;\n      So overall not a lot of action &#8230; our lab was actually the one with larges effect size (in the predicted direction).\n    &lt;/p&gt;\n    \n    &lt;p&gt;\n      Here is the abstract of the whole paper and here the¬†&lt;a href=\"https://www.psychologicalscience.org/redesign/wp-content/uploads/2017/11/Dijksterhuis_RRRcommentary_ACPT.pdf\"&gt;Commentary by¬†Ap Dijksterhuis&lt;/a&gt;¬†naturally, he sees things a bit different:           Dijksterhuis and van Knippenberg (1998) reported that participants primed with an intelligent category (‚Äúprofessor‚Äù) subsequently performed 13.1% better on a trivia test than participants primed with an unintelligent category (‚Äúsoccer hooligans‚Äù). Two unpublished replications of this study by the original authors, designed to verify the appropriate testing procedures, observed a smaller difference between conditions (2-3%) as well as a gender difference: men showed the effect (9.3% and 7.6%) but women did not (0.3% and -0.3%). The procedure used in those replications served as the basis for this multi-lab Registered Replication Report (RRR). A total of 40 laboratories collected data for this project, with 23 laboratories meeting all inclusion criteria. Here we report the meta-analytic result of those 23 direct replications (total N = 4,493) of the updated version of the original study, examining the difference between priming with professor and hooligan on a 30-item general knowledge trivia task (a supplementary analysis reports results with all 40 labs, N = 6,454). We observed no overall difference in trivia performance between participants primed with professor and those primed with hooligan (0.14%) and no moderation by gender.\n    &lt;/p&gt;\n  &lt;/div&gt;"
  },
  {
    "objectID": "posts/2009-06-17-xampp-activating-mysql.html",
    "href": "posts/2009-06-17-xampp-activating-mysql.html",
    "title": "XAMPP activating mysql",
    "section": "",
    "text": "Here is the problem: I want to have a server (Apache, MySQL) on my local machine (for the current purpose it was a Windows machine) to do some developing, testing or a demo. Getting XAMPP at Sourceforge is straight forward, installing it works like a charm.\nBUT\nA problem coming back from time to time is the following: the MySQL daemon does not start.\nHmmmm\nHere are some steps that helped me:\n\nmake sure to deactivate any services related to MySQL and Apache\n\nshutdown XAMPP\n\nto be sure I restart the machine\n\ndo not start XAMPP with the .bat file but use the MySQL and Apache batch scripts ‚Äì these open a command window which you should not close as long as you use XAMPP\n\nThrough these steps everything worked fine ‚Ä¶"
  },
  {
    "objectID": "posts/2017-03-13-eye-tracking-with-n-1.html",
    "href": "posts/2017-03-13-eye-tracking-with-n-1.html",
    "title": "Eye-Tracking with N > 1",
    "section": "",
    "text": "This is one of the fastest papers I have ever written. It was a great collaboration with Tom√°s Lejarraga¬†from the Universitat de les Illes Balears. Why was it great? Because it is one of the rare cases (at least in my academic life) where all people involved in a project contribute equally and quickly. Often, the weight of a contribution lies with one person¬†which slows down things ‚Äì with Tom√°s this was different ‚Äì we were often sitting in front of a computer writing together (have never done this before, thought it would not work). Surprisingly this collaborative writing worked out very well and we had the skeleton of the paper within an afternoon. This was followed by many hours of tuning and tacking turns ‚Äì but in principle we wrote the most important parts together ‚Äì which was pretty cool.\nEven cooler ‚Äì you can do eye-tracking in groups, using our code.\nHere is the [PDF] and abstract:\nThe recent introduction of inexpensive eye-trackers has opened up a wealth of opportunities for researchers to study attention in interactive tasks. No software package was previously available to help researchers exploit those opportunities. We created ‚Äúthe pyeTribe‚Äù, a software package that offers, among others, the following features: First, a communication platform between many eye-trackers to allow simultaneous recording of multiple participants. Second, the simultaneous calibration of multiple eye-trackers without the experimenter‚Äôs supervision. Third, data collection restricted to periods of interest, thus reducing the volume of data and easing analysis. We used a standard economic game (the public goods game) to examine data quality and demonstrate the potential of our software package. Moreover, we conducted a modeling analysis, which illustrates how combining process and behavioral data can improve models of human decision making behavior in social situations. Our software is open source and can thus be used and improved by others."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael Schulte-Mecklenbeck",
    "section": "",
    "text": "Kinderuni\n\n\n\n\n\n\n\nTeaching\n\n\nDecision Making\n\n\nAnchoring\n\n\nWisdom of crowds\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nReplication: Default effect\n\n\n\n\n\n\n\nReplication\n\n\nDefault\n\n\nFraming\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nReplication: Kisses\n\n\n\n\n\n\n\nReplication\n\n\nEmotion\n\n\nChoice\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nReplication: Numeracy and decision making\n\n\n\n\n\n\n\nReplication\n\n\nFraming\n\n\nNumeracy\n\n\nFailedreplication\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nReplication: In search of homo economicus\n\n\n\n\n\n\n\nReplication\n\n\nChoice\n\n\nEmotion\n\n\nFailedreplication\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2022\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nThings to come in Quarto\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nOpen Science Lecture Series\n\n\n\n\n\n\n\nOpen science\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2020\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nWhat HoPTM looks like from the inside\n\n\n\n\n\n\n\nProcess Tracing\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2019\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nHandbook of Process Tracing Methods\n\n\n\n\n\n\n\nProcess tracing\n\n\nDecision Making\n\n\nEye tracking\n\n\nFlashlight\n\n\nMousetracking\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2019\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nPictures\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2019\n\n\nMSM\n\n\n\n\n\n\n  \n\n\n\n\nBernR Meetup\n\n\n\n\n\n\n\nProcess Tracing\n\n\nHandbook\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2019\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nBernR Meetup\n\n\n\n\n\n\n\nMeetup\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nA letter to the black goat\n\n\n\n\n\n\n\nOpen science\n\n\nReplication\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2018\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nBlind Haste (aka im Blindflug)\n\n\n\n\n\n\n\nPerception\n\n\nTraffic psychology\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2018\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nProfessor priming - or not\n\n\n\n\n\n\n\nDecision Making\n\n\nReplication\n\n\nRRR\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2017\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nThe root of the problem\n\n\n\n\n\n\n\nReplication\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2017\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nA short history of process tracing\n\n\n\n\n\n\n\nDecision Making\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2017\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nLaTeX &lt;- pdfTeX, LuaTeX, XeTeX\n\n\n\n\n\n\n\nLaTeX\n\n\nLifehacks\n\n\nScience\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2017\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nGrowing up to be old\n\n\n\n\n\n\n\nDecision Making\n\n\nProcess tracing\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2017\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nSomething about reverse inference\n\n\n\n\n\n\n\nDecision Making\n\n\nggplot\n\n\nProcess tracing\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2017\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nEye-Tracking with N &gt; 1\n\n\n\n\n\n\n\nEye tracking\n\n\nMethods\n\n\nOpen Source\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2017\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nEverything you believe in is wrong ‚Äì or is it simply terrorism?\n\n\n\n\n\n\n\nReplication\n\n\nScience\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2016\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nEverything is fucked ‚Ä¶\n\n\n\n\n\n\n\nEgo depletion\n\n\nReplication\n\n\nScience\n\n\nTeaching\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2016\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nBefore R there was S\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2016\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nThe exams package\n\n\n\n\n\n\n\nR\n\n\nTeaching\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2016\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nSublime autocompletion\n\n\n\n\n\n\n\nLaTeX\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2016\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nThree weeks without email\n\n\n\n\n\n\n\nLifehacks\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2016\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nNew Paper on pychodiagnosis and eye-tracking\n\n\n\n\n\n\n\nDecision Making\n\n\nEye tracking\n\n\nMethods\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2015\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nthe way I am seen, by people who know (and like) me\n\n\n\n\n\n\n\nDecision Making\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2015\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nabout illusions\n\n\n\n\n\n\n\nDecision Making\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2015\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nSchnitz(e)ljagd\n\n\n\n\n\n\n\nFood Choice\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2015\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nall that mutate() and summarise() beauty\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2015\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nMoving an idea into business\n\n\n\n\n\n\n\nDecision Making\n\n\nFlashlight\n\n\nOpen Source\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2015\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\ndplyr is growing up ‚Ä¶\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2014\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nDie Zeit Wissen ‚Ä¶ schreibt √ºber Essensentscheidungen\n\n\n\n\n\n\n\nDecision Making\n\n\nFood Choice\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2014\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nWhen something old ‚Ä¶\n\n\n\n\n\n\n\nDecision Making\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2014\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nNew paper on food choice and simple heuristics\n\n\n\n\n\n\n\nDecision Making\n\n\nFood Choice\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2013\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nLimesurvey randomizing\n\n\n\n\n\n\n\nLimesurvey\n\n\nOpen Source\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2013\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\n*apply in all its variations ‚Ä¶\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2012\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nR Style Guide\n\n\n\n\n\n\n\nGoogle\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2012\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nReligion and # of babies ‚Ä¶\n\n\n\n\n\n\n\nGapminder\n\n\nGoogle\n\n\nOpen Source\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2012\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nOn writing in social science\n\n\n\n\n\n\n\nDecision Making\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2012\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nhow decisions deplete and breaks help\n\n\n\n\n\n\n\nDecision Making\n\n\nEgo depletion\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2011\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nWhy anybody should learn/use R ‚Ä¶\n\n\n\n\n\n\n\nMethods\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2011\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nawesome visualization tool for R\n\n\n\n\n\n\n\nGoogle\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2011\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nresinstalling packages in R after update\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2011\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nSustainable research\n\n\n\n\n\n\n\nOpen Source\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2011\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nPsychology as a reproducible Science\n\n\n\n\n\n\n\nLaTeX\n\n\nMethods\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2011\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nBlog-attack\n\n\n\n\n\n\n\nDecision Making\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2011\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nWhere you end up, having a PhD\n\n\n\n\n\n\n\nScience\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2010\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\ntwo good things come together\n\n\n\n\n\n\n\nLaTeX\n\n\nTech\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2010\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nR and the World Cup\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2010\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nR goes cloud\n\n\n\n\n\n\n\nMethods\n\n\nR\n\n\nStatistics\n\n\nTech\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2010\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nLaTeX looks more scientific\n\n\n\n\n\n\n\nLaTeX\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2010\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nAccepting to fail (in the name of science)\n\n\n\n\n\n\n\nScience\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nHow WEIRD subjects can be overcome ‚Ä¶ a comment on Henrich et al.\n\n\n\n\n\n\n\nDecision Making\n\n\nMethods\n\n\nScience\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nLattice versus ggplot2\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nR flashmob\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nFlashlight paper draft\n\n\n\n\n\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nLaTeX tips\n\n\n\n\n\n\n\nLaTeX\n\n\nTech\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nXAMPP activating mysql\n\n\n\n\n\n\n\nTech\n\n\nXAMPP\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\ndouble, triple or even sextuple blind?\n\n\n\n\n\n\n\nReview process\n\n\nScience\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nInference and R\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\ncreate colored title in R plot\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2009\n\n\nMichael\n\n\n\n\n\n\n  \n\n\n\n\nPriority Heuristic comment\n\n\n\n\n\n\n\nHeuristics\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2008\n\n\nMSM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I study how people make decisions. More specifically, I am interested in what (cognitive) processes happen, when we make decisions and how we can measure all of that. Within these lines of research I developed my own tools to study decision processes (WebDiP, Flashlight, The pyeTribe) and want to understand decisions in the lab, under controlled circumstances as well as in the wild.\nFurthermore I am interested in open [science|data|methods], replication studies and new (and old) teaching methods."
  },
  {
    "objectID": "about.html#what-i-am-interested-in",
    "href": "about.html#what-i-am-interested-in",
    "title": "About",
    "section": "",
    "text": "I study how people make decisions. More specifically, I am interested in what (cognitive) processes happen, when we make decisions and how we can measure all of that. Within these lines of research I developed my own tools to study decision processes (WebDiP, Flashlight, The pyeTribe) and want to understand decisions in the lab, under controlled circumstances as well as in the wild.\nFurthermore I am interested in open [science|data|methods], replication studies and new (and old) teaching methods."
  },
  {
    "objectID": "about.html#current-position",
    "href": "about.html#current-position",
    "title": "About",
    "section": "Current position",
    "text": "Current position\nUniversity of Bern, Switzerland\nMax-Planck-Institute for Human Development in Berlin."
  }
]